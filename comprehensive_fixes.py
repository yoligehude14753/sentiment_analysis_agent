# -*- coding: utf-8 -*-

import sqlite3
import json
import hashlib
from difflib import SequenceMatcher
from typing import List, Dict, Tuple, Optional
import logging
from datetime import datetime
import uuid
from ali_llm_client import AliLLMClient

logger = logging.getLogger(__name__)

class ComprehensiveFixes:
    """ç»¼åˆä¿®å¤å·¥å…·ç±»"""
    
    def __init__(self, db_path="data/analysis_results.db"):
        self.db_path = db_path
        self.llm_client = AliLLMClient()
    
    def fix_empty_summaries(self):
        """ä¿®å¤ç©ºæ‘˜è¦é—®é¢˜"""
        print("ğŸ”§ å¼€å§‹ä¿®å¤ç©ºæ‘˜è¦...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æŸ¥æ‰¾æ‘˜è¦ä¸ºç©ºæˆ–ä¸º"æ— æ‘˜è¦"çš„è®°å½•
                cursor.execute('''
                    SELECT id, content, summary FROM sentiment_results 
                    WHERE summary IS NULL OR summary = '' OR summary = 'æ— æ‘˜è¦'
                    ORDER BY id DESC
                    LIMIT 50
                ''')
                
                records = cursor.fetchall()
                print(f"æ‰¾åˆ° {len(records)} æ¡éœ€è¦ä¿®å¤æ‘˜è¦çš„è®°å½•")
                
                fixed_count = 0
                for record_id, content, old_summary in records:
                    if not content or len(content.strip()) < 10:
                        continue
                    
                    try:
                        # ä½¿ç”¨AIç”Ÿæˆæ‘˜è¦
                        new_summary = self.llm_client.generate_summary(content)
                        
                        # å¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æˆªå–æ‘˜è¦
                        if "æ‘˜è¦ç”Ÿæˆå¤±è´¥" in new_summary or not new_summary.strip():
                            new_summary = content[:200] + "..." if len(content) > 200 else content
                        
                        # æ›´æ–°æ•°æ®åº“
                        cursor.execute('''
                            UPDATE sentiment_results 
                            SET summary = ? 
                            WHERE id = ?
                        ''', (new_summary, record_id))
                        
                        fixed_count += 1
                        print(f"âœ… ä¿®å¤è®°å½• {record_id} çš„æ‘˜è¦")
                        
                    except Exception as e:
                        print(f"âŒ ä¿®å¤è®°å½• {record_id} å¤±è´¥: {str(e)}")
                        # ä½¿ç”¨æˆªå–æ‘˜è¦ä½œä¸ºå¤‡é€‰
                        fallback_summary = content[:200] + "..." if len(content) > 200 else content
                        cursor.execute('''
                            UPDATE sentiment_results 
                            SET summary = ? 
                            WHERE id = ?
                        ''', (fallback_summary, record_id))
                
                conn.commit()
                print(f"ğŸ‰ æ‘˜è¦ä¿®å¤å®Œæˆï¼Œå…±ä¿®å¤ {fixed_count} æ¡è®°å½•")
                
        except Exception as e:
            print(f"âŒ ä¿®å¤æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
    
    def calculate_content_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå†…å®¹çš„ç›¸ä¼¼åº¦"""
        if not content1 or not content2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, content1.strip(), content2.strip()).ratio()
        return round(similarity, 3)
    
    def generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼ç”¨äºå¿«é€Ÿæ¯”è¾ƒ"""
        if not content:
            return ""
        return hashlib.md5(content.strip().encode('utf-8')).hexdigest()[:16]
    
    def detect_duplicates_and_update(self, similarity_threshold: float = 0.8):
        """æ£€æµ‹é‡å¤æ•°æ®å¹¶æ›´æ–°duplicate_idå’Œduplication_rateå­—æ®µ"""
        print(f"ğŸ” å¼€å§‹æ£€æµ‹é‡å¤æ•°æ®ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}ï¼‰...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # è·å–æ‰€æœ‰è®°å½•
                cursor.execute('''
                    SELECT id, original_id, title, content, duplicate_id, duplication_rate 
                    FROM sentiment_results 
                    ORDER BY id
                ''')
                
                records = cursor.fetchall()
                print(f"å…±æœ‰ {len(records)} æ¡è®°å½•éœ€è¦æ£€æµ‹")
                
                # æ„å»ºå†…å®¹å“ˆå¸Œæ˜ å°„
                hash_groups = {}
                for record in records:
                    record_id, original_id, title, content, _, _ = record
                    content_hash = self.generate_content_hash(content)
                    
                    if content_hash not in hash_groups:
                        hash_groups[content_hash] = []
                    hash_groups[content_hash].append(record)
                
                updated_count = 0
                duplicate_groups = []
                
                # æ£€æµ‹æ¯ä¸ªå“ˆå¸Œç»„å†…çš„é‡å¤
                for content_hash, group_records in hash_groups.items():
                    if len(group_records) <= 1:
                        continue
                    
                    # è¯¦ç»†ç›¸ä¼¼åº¦æ£€æµ‹
                    for i, record1 in enumerate(group_records):
                        for j, record2 in enumerate(group_records[i+1:], i+1):
                            similarity = self.calculate_content_similarity(record1[3], record2[3])
                            
                            if similarity >= similarity_threshold:
                                # æ‰¾åˆ°é‡å¤æ•°æ®
                                duplicate_groups.append({
                                    'record1': record1,
                                    'record2': record2,
                                    'similarity': similarity
                                })
                
                print(f"æ‰¾åˆ° {len(duplicate_groups)} å¯¹é‡å¤æ•°æ®")
                
                # æ›´æ–°é‡å¤æ•°æ®å­—æ®µ
                for dup_group in duplicate_groups:
                    record1 = dup_group['record1']
                    record2 = dup_group['record2']
                    similarity = dup_group['similarity']
                    
                    # æ›´æ–°ç¬¬ä¸€æ¡è®°å½•
                    cursor.execute('''
                        UPDATE sentiment_results 
                        SET duplicate_id = ?, duplication_rate = ?
                        WHERE id = ?
                    ''', (str(record2[0]), similarity, record1[0]))
                    
                    # æ›´æ–°ç¬¬äºŒæ¡è®°å½•
                    cursor.execute('''
                        UPDATE sentiment_results 
                        SET duplicate_id = ?, duplication_rate = ?
                        WHERE id = ?
                    ''', (str(record1[0]), similarity, record2[0]))
                    
                    updated_count += 2
                    print(f"âœ… æ ‡è®°é‡å¤: ID {record1[0]} <-> ID {record2[0]} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                
                conn.commit()
                print(f"ğŸ‰ é‡å¤æ£€æµ‹å®Œæˆï¼Œæ›´æ–°äº† {updated_count} æ¡è®°å½•")
                
                return len(duplicate_groups)
                
        except Exception as e:
            print(f"âŒ æ£€æµ‹é‡å¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return 0
    
    def clean_duplicate_records(self, keep_strategy: str = "first"):
        """æ¸…ç†é‡å¤è®°å½•"""
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç†é‡å¤è®°å½•ï¼ˆä¿ç•™ç­–ç•¥: {keep_strategy}ï¼‰...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æŸ¥æ‰¾æœ‰é‡å¤æ ‡è®°çš„è®°å½•
                cursor.execute('''
                    SELECT id, original_id, duplicate_id, duplication_rate, analysis_time
                    FROM sentiment_results 
                    WHERE duplicate_id IS NOT NULL AND duplicate_id != '' AND duplicate_id != 'æ— '
                    ORDER BY id
                ''')
                
                duplicate_records = cursor.fetchall()
                print(f"æ‰¾åˆ° {len(duplicate_records)} æ¡é‡å¤è®°å½•")
                
                if not duplicate_records:
                    print("æ²¡æœ‰æ‰¾åˆ°é‡å¤è®°å½•")
                    return 0
                
                # æŒ‰é‡å¤ç»„åˆ†ç»„
                duplicate_groups = {}
                for record in duplicate_records:
                    record_id, original_id, duplicate_id, duplication_rate, analysis_time = record
                    
                    # åˆ›å»ºé‡å¤ç»„çš„é”®ï¼ˆä½¿ç”¨è¾ƒå°çš„IDä½œä¸ºç»„é”®ï¼‰
                    group_key = tuple(sorted([record_id, int(duplicate_id)]))
                    
                    if group_key not in duplicate_groups:
                        duplicate_groups[group_key] = []
                    duplicate_groups[group_key].append(record)
                
                deleted_count = 0
                for group_key, group_records in duplicate_groups.items():
                    if len(group_records) <= 1:
                        continue
                    
                    # æ ¹æ®ç­–ç•¥é€‰æ‹©ä¿ç•™çš„è®°å½•
                    if keep_strategy == "first":
                        # ä¿ç•™IDæœ€å°çš„
                        keep_record = min(group_records, key=lambda x: x[0])
                    elif keep_strategy == "latest":
                        # ä¿ç•™æœ€æ–°çš„
                        keep_record = max(group_records, key=lambda x: x[4] or "")
                    else:
                        keep_record = group_records[0]
                    
                    # åˆ é™¤å…¶ä»–è®°å½•
                    for record in group_records:
                        if record[0] != keep_record[0]:
                            cursor.execute('DELETE FROM sentiment_results WHERE id = ?', (record[0],))
                            deleted_count += 1
                            print(f"ğŸ—‘ï¸  åˆ é™¤é‡å¤è®°å½• ID: {record[0]}")
                
                conn.commit()
                print(f"ğŸ‰ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} æ¡é‡å¤è®°å½•")
                
                return deleted_count
                
        except Exception as e:
            print(f"âŒ æ¸…ç†é‡å¤è®°å½•æ—¶å‡ºé”™: {str(e)}")
            return 0
    
    def export_deduplicated_data(self, session_id: Optional[str] = None, output_file: str = None):
        """å¯¼å‡ºå»é‡åçš„æ•°æ®"""
        print("ğŸ“¤ å¼€å§‹å¯¼å‡ºå»é‡åçš„æ•°æ®...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                if session_id:
                    query = '''
                        SELECT * FROM sentiment_results 
                        WHERE session_id = ?
                        ORDER BY id DESC
                    '''
                    cursor.execute(query, (session_id,))
                else:
                    query = '''
                        SELECT * FROM sentiment_results 
                        ORDER BY id DESC
                    '''
                    cursor.execute(query)
                
                columns = [description[0] for description in cursor.description]
                rows = cursor.fetchall()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                all_data = []
                for row in rows:
                    data_dict = dict(zip(columns, row))
                    all_data.append(data_dict)
                
                print(f"ä»æ•°æ®åº“è·å–äº† {len(all_data)} æ¡è®°å½•")
                
                # åŸºäºå†…å®¹å»é‡
                seen_hashes = set()
                deduplicated_data = []
                
                for data in all_data:
                    content = data.get('content', '')
                    content_hash = self.generate_content_hash(content)
                    
                    if content_hash not in seen_hashes:
                        seen_hashes.add(content_hash)
                        deduplicated_data.append(data)
                    else:
                        print(f"è·³è¿‡é‡å¤è®°å½• ID: {data.get('id')}")
                
                print(f"å»é‡åå‰©ä½™ {len(deduplicated_data)} æ¡è®°å½•")
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                if not output_file:
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    output_file = f"deduplicated_analysis_results_{timestamp}.json"
                
                # å¯¼å‡ºåˆ°JSONæ–‡ä»¶
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(deduplicated_data, f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ‰ å»é‡æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
                
                return {
                    'success': True,
                    'total_records': len(all_data),
                    'deduplicated_records': len(deduplicated_data),
                    'removed_duplicates': len(all_data) - len(deduplicated_data),
                    'output_file': output_file
                }
                
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå»é‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def get_database_stats(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æ€»è®°å½•æ•°
                cursor.execute('SELECT COUNT(*) FROM sentiment_results')
                total_records = cursor.fetchone()[0]
                
                # æœ‰æ‘˜è¦çš„è®°å½•æ•°
                cursor.execute('''
                    SELECT COUNT(*) FROM sentiment_results 
                    WHERE summary IS NOT NULL AND summary != '' AND summary != 'æ— æ‘˜è¦'
                ''')
                records_with_summary = cursor.fetchone()[0]
                
                # æ ‡è®°ä¸ºé‡å¤çš„è®°å½•æ•°
                cursor.execute('''
                    SELECT COUNT(*) FROM sentiment_results 
                    WHERE duplicate_id IS NOT NULL AND duplicate_id != '' AND duplicate_id != 'æ— '
                ''')
                duplicate_records = cursor.fetchone()[0]
                
                # æœ€è¿‘çš„è®°å½•
                cursor.execute('''
                    SELECT analysis_time FROM sentiment_results 
                    ORDER BY id DESC LIMIT 1
                ''')
                latest_record = cursor.fetchone()
                latest_time = latest_record[0] if latest_record else "æ— "
                
                stats = {
                    'total_records': total_records,
                    'records_with_summary': records_with_summary,
                    'records_without_summary': total_records - records_with_summary,
                    'duplicate_records': duplicate_records,
                    'latest_record_time': latest_time
                }
                
                print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
                print(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
                print(f"  æœ‰æ‘˜è¦è®°å½•: {stats['records_with_summary']}")
                print(f"  æ— æ‘˜è¦è®°å½•: {stats['records_without_summary']}")
                print(f"  é‡å¤è®°å½•: {stats['duplicate_records']}")
                print(f"  æœ€æ–°è®°å½•æ—¶é—´: {stats['latest_record_time']}")
                
                return stats
                
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            return {}

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œæ‰€æœ‰ä¿®å¤"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œç»¼åˆä¿®å¤...")
    
    fixer = ComprehensiveFixes()
    
    # 1. è·å–å½“å‰ç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š ä¿®å¤å‰ç»Ÿè®¡")
    print("="*50)
    fixer.get_database_stats()
    
    # 2. ä¿®å¤ç©ºæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ”§ ä¿®å¤ç©ºæ‘˜è¦")
    print("="*50)
    fixer.fix_empty_summaries()
    
    # 3. æ£€æµ‹å¹¶æ ‡è®°é‡å¤æ•°æ®
    print("\n" + "="*50)
    print("ğŸ” æ£€æµ‹é‡å¤æ•°æ®")
    print("="*50)
    duplicate_count = fixer.detect_duplicates_and_update()
    
    # 4. è·å–ä¿®å¤åç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š ä¿®å¤åç»Ÿè®¡")
    print("="*50)
    stats = fixer.get_database_stats()
    
    # 5. å¯¼å‡ºå»é‡æ•°æ®
    print("\n" + "="*50)
    print("ğŸ“¤ å¯¼å‡ºå»é‡æ•°æ®")
    print("="*50)
    export_result = fixer.export_deduplicated_data()
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ‰ ä¿®å¤å®Œæˆæ€»ç»“")
    print("="*50)
    print(f"âœ… ä¿®å¤äº†ç©ºæ‘˜è¦é—®é¢˜")
    print(f"âœ… æ£€æµ‹åˆ° {duplicate_count} å¯¹é‡å¤æ•°æ®")
    print(f"âœ… æ ‡è®°äº†é‡å¤è®°å½•")
    if export_result.get('success'):
        print(f"âœ… å¯¼å‡ºäº†å»é‡æ•°æ®: {export_result.get('output_file')}")
        print(f"   - åŸå§‹è®°å½•: {export_result.get('total_records')}")
        print(f"   - å»é‡å: {export_result.get('deduplicated_records')}")
        print(f"   - ç§»é™¤é‡å¤: {export_result.get('removed_duplicates')}")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    if stats.get('duplicate_records', 0) > 0:
        print("   - å¯ä»¥è¿è¡Œ fixer.clean_duplicate_records() æ¸…ç†é‡å¤è®°å½•")
    print("   - ä½¿ç”¨å»é‡åçš„å¯¼å‡ºæ–‡ä»¶ä½œä¸ºæœ€ç»ˆç»“æœ")

if __name__ == "__main__":
    main()
