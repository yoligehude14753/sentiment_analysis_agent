#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªåŠ¨å»é‡æ¨¡å—
åœ¨æ•°æ®å¯¼å‡ºå‰è‡ªåŠ¨æ‰§è¡Œå»é‡å¤„ç†ï¼Œç¡®ä¿å¯¼å‡ºçš„æ•°æ®ä¸åŒ…å«é‡å¤é¡¹
"""

import logging
import sqlite3
import json
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
import hashlib
import re
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

class AutoDeduplicator:
    """
    è‡ªåŠ¨å»é‡å™¨
    åœ¨å¯¼å‡ºæ•°æ®å‰è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†é‡å¤æ•°æ®
    """
    
    def __init__(self, db_path: str = "data/analysis_results.db"):
        self.db_path = db_path
        self.similarity_threshold = 0.85  # é»˜è®¤ç›¸ä¼¼åº¦é˜ˆå€¼
        
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # æ¸…ç†æ–‡æœ¬
        clean_text1 = self._clean_text(text1)
        clean_text2 = self._clean_text(text2)
        
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, clean_text1, clean_text2).ratio()
        return similarity
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºç™½"""
        if not text:
            return ""
        
        # å»é™¤HTMLæ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', text)
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text.strip())
        return clean_text
    
    def _generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼ç”¨äºå¿«é€Ÿæ¯”è¾ƒ"""
        if not content:
            return ""
        
        clean_content = self._clean_text(content)
        return hashlib.md5(clean_content.encode('utf-8')).hexdigest()
    
    def detect_duplicates_in_data(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åœ¨ç»™å®šæ•°æ®ä¸­æ£€æµ‹é‡å¤é¡¹
        
        Args:
            data: è¦æ£€æµ‹çš„æ•°æ®åˆ—è¡¨
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«é‡å¤ç»„ä¿¡æ¯å’Œç»Ÿè®¡
        """
        logger.info(f"ğŸ” å¼€å§‹æ£€æµ‹ {len(data)} æ¡è®°å½•ä¸­çš„é‡å¤æ•°æ®...")
        
        # æŒ‰original_idåˆ†ç»„è¿›è¡Œåˆæ­¥ç­›é€‰
        original_id_groups = defaultdict(list)
        content_hash_groups = defaultdict(list)
        
        for i, item in enumerate(data):
            original_id = item.get('original_id')
            content = item.get('content', '')
            
            # æŒ‰original_idåˆ†ç»„
            if original_id:
                original_id_groups[original_id].append((i, item))
            
            # æŒ‰å†…å®¹å“ˆå¸Œåˆ†ç»„
            content_hash = self._generate_content_hash(content)
            if content_hash:
                content_hash_groups[content_hash].append((i, item))
        
        duplicate_groups = []
        duplicate_indices = set()
        
        # 1. æ£€æµ‹ç›¸åŒoriginal_idçš„é‡å¤
        for original_id, group in original_id_groups.items():
            if len(group) > 1:
                logger.debug(f"å‘ç° original_id {original_id} æœ‰ {len(group)} æ¡é‡å¤è®°å½•")
                
                # ä¿ç•™ç¬¬ä¸€æ¡ï¼Œå…¶ä»–æ ‡è®°ä¸ºé‡å¤
                primary_index = group[0][0]
                duplicate_items = []
                
                for i, (index, item) in enumerate(group[1:], 1):
                    duplicate_indices.add(index)
                    duplicate_items.append({
                        'index': index,
                        'item': item,
                        'reason': f'ç›¸åŒoriginal_id: {original_id}',
                        'similarity': 1.0
                    })
                
                if duplicate_items:
                    duplicate_groups.append({
                        'primary_index': primary_index,
                        'primary_item': group[0][1],
                        'duplicates': duplicate_items,
                        'type': 'original_id_duplicate'
                    })
        
        # 2. æ£€æµ‹å†…å®¹ç›¸ä¼¼çš„é‡å¤ï¼ˆæ’é™¤å·²ç»é€šè¿‡original_idå‘ç°çš„é‡å¤ï¼‰
        for content_hash, group in content_hash_groups.items():
            if len(group) > 1:
                # è¿‡æ»¤æ‰å·²ç»æ ‡è®°ä¸ºé‡å¤çš„é¡¹
                filtered_group = [(i, item) for i, item in group if i not in duplicate_indices]
                
                if len(filtered_group) > 1:
                    logger.debug(f"å‘ç°å†…å®¹å“ˆå¸Œ {content_hash[:8]}... æœ‰ {len(filtered_group)} æ¡å¯èƒ½é‡å¤çš„è®°å½•")
                    
                    # è¿›è¡Œè¯¦ç»†çš„ç›¸ä¼¼åº¦æ£€æµ‹
                    for j, (index1, item1) in enumerate(filtered_group):
                        for index2, item2 in filtered_group[j+1:]:
                            if index2 in duplicate_indices:
                                continue
                                
                            similarity = self.calculate_text_similarity(
                                item1.get('content', ''), 
                                item2.get('content', '')
                            )
                            
                            if similarity >= self.similarity_threshold:
                                duplicate_indices.add(index2)
                                
                                # æ‰¾åˆ°æˆ–åˆ›å»ºé‡å¤ç»„
                                existing_group = None
                                for group in duplicate_groups:
                                    if group['primary_index'] == index1:
                                        existing_group = group
                                        break
                                
                                if existing_group:
                                    existing_group['duplicates'].append({
                                        'index': index2,
                                        'item': item2,
                                        'reason': f'å†…å®¹ç›¸ä¼¼åº¦: {similarity:.2f}',
                                        'similarity': similarity
                                    })
                                else:
                                    duplicate_groups.append({
                                        'primary_index': index1,
                                        'primary_item': item1,
                                        'duplicates': [{
                                            'index': index2,
                                            'item': item2,
                                            'reason': f'å†…å®¹ç›¸ä¼¼åº¦: {similarity:.2f}',
                                            'similarity': similarity
                                        }],
                                        'type': 'content_similarity_duplicate'
                                    })
        
        total_duplicates = len(duplicate_indices)
        logger.info(f"âœ… é‡å¤æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(duplicate_groups)} ä¸ªé‡å¤ç»„ï¼Œå…± {total_duplicates} æ¡é‡å¤è®°å½•")
        
        return {
            'duplicate_groups': duplicate_groups,
            'duplicate_indices': duplicate_indices,
            'total_records': len(data),
            'total_duplicates': total_duplicates,
            'unique_records': len(data) - total_duplicates,
            'detection_time': datetime.now().isoformat()
        }
    
    def remove_duplicates_from_data(self, data: List[Dict[str, Any]], 
                                  detection_result: Dict[str, Any] = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        ä»æ•°æ®ä¸­ç§»é™¤é‡å¤é¡¹
        
        Args:
            data: åŸå§‹æ•°æ®åˆ—è¡¨
            detection_result: å¯é€‰çš„æ£€æµ‹ç»“æœï¼Œå¦‚æœä¸æä¾›åˆ™é‡æ–°æ£€æµ‹
            
        Returns:
            (å»é‡åçš„æ•°æ®, å»é‡ç»Ÿè®¡ä¿¡æ¯)
        """
        if detection_result is None:
            detection_result = self.detect_duplicates_in_data(data)
        
        duplicate_indices = detection_result['duplicate_indices']
        
        # åˆ›å»ºå»é‡åçš„æ•°æ®åˆ—è¡¨
        deduplicated_data = []
        removed_items = []
        
        for i, item in enumerate(data):
            if i in duplicate_indices:
                removed_items.append({
                    'index': i,
                    'item': item,
                    'original_id': item.get('original_id'),
                    'title': item.get('title', '')[:50] + '...' if item.get('title') else 'No title'
                })
            else:
                deduplicated_data.append(item)
        
        stats = {
            'original_count': len(data),
            'removed_count': len(removed_items),
            'final_count': len(deduplicated_data),
            'duplicate_groups': len(detection_result['duplicate_groups']),
            'removed_items': removed_items,
            'processing_time': datetime.now().isoformat()
        }
        
        logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: åŸå§‹ {stats['original_count']} æ¡ â†’ å»é‡å {stats['final_count']} æ¡ (ç§»é™¤ {stats['removed_count']} æ¡)")
        
        return deduplicated_data, stats
    
    def auto_deduplicate_export_data(self, data: List[Dict[str, Any]], 
                                   export_options: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        è‡ªåŠ¨å»é‡å¯¼å‡ºæ•°æ®
        è¿™æ˜¯ä¸»è¦çš„æ¥å£å‡½æ•°ï¼Œåœ¨å¯¼å‡ºå‰è°ƒç”¨
        
        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®
            export_options: å¯¼å‡ºé€‰é¡¹
            
        Returns:
            åŒ…å«å»é‡åæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        start_time = datetime.now()
        logger.info(f"ğŸš€ å¼€å§‹è‡ªåŠ¨å»é‡å¤„ç†ï¼ŒåŸå§‹æ•°æ®é‡: {len(data)} æ¡")
        
        if not data:
            return {
                'success': True,
                'data': [],
                'stats': {
                    'original_count': 0,
                    'final_count': 0,
                    'removed_count': 0,
                    'processing_time': 0
                },
                'message': 'æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†'
            }
        
        try:
            # 1. æ£€æµ‹é‡å¤
            detection_result = self.detect_duplicates_in_data(data)
            
            # 2. ç§»é™¤é‡å¤é¡¹
            deduplicated_data, removal_stats = self.remove_duplicates_from_data(data, detection_result)
            
            # 3. ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡
            processing_time = (datetime.now() - start_time).total_seconds()
            
            final_stats = {
                **removal_stats,
                'detection_result': detection_result,
                'processing_time_seconds': processing_time,
                'similarity_threshold': self.similarity_threshold,
                'deduplication_rate': removal_stats['removed_count'] / removal_stats['original_count'] if removal_stats['original_count'] > 0 else 0
            }
            
            logger.info(f"âœ… è‡ªåŠ¨å»é‡å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f}sï¼Œå»é‡ç‡ {final_stats['deduplication_rate']:.2%}")
            
            return {
                'success': True,
                'data': deduplicated_data,
                'stats': final_stats,
                'message': f'å»é‡å®Œæˆï¼š{final_stats["original_count"]} â†’ {final_stats["final_count"]} æ¡è®°å½•'
            }
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨å»é‡å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'data': data,  # è¿”å›åŸå§‹æ•°æ®
                'stats': {
                    'original_count': len(data),
                    'final_count': len(data),
                    'removed_count': 0,
                    'error': str(e)
                },
                'message': f'å»é‡å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®: {str(e)}'
            }
    
    def get_duplicate_summary_report(self, data: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆé‡å¤æ•°æ®æ‘˜è¦æŠ¥å‘Š
        
        Args:
            data: è¦åˆ†æçš„æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        detection_result = self.detect_duplicates_in_data(data)
        
        report_lines = [
            "=" * 60,
            "ğŸ“Š é‡å¤æ•°æ®æ£€æµ‹æŠ¥å‘Š",
            "=" * 60,
            f"æ€»è®°å½•æ•°: {detection_result['total_records']}",
            f"é‡å¤è®°å½•æ•°: {detection_result['total_duplicates']}",
            f"å”¯ä¸€è®°å½•æ•°: {detection_result['unique_records']}",
            f"é‡å¤ç»„æ•°: {len(detection_result['duplicate_groups'])}",
            f"é‡å¤ç‡: {detection_result['total_duplicates'] / detection_result['total_records']:.2%}",
            "",
            "ğŸ” é‡å¤ç»„è¯¦æƒ…:"
        ]
        
        for i, group in enumerate(detection_result['duplicate_groups'][:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ªç»„
            primary_item = group['primary_item']
            primary_title = primary_item.get('title', 'No title')[:30] + '...'
            
            report_lines.append(f"  ç»„ {i}: {primary_title}")
            report_lines.append(f"    ä¸»è®°å½•: original_id={primary_item.get('original_id')}")
            
            for dup in group['duplicates'][:3]:  # æ¯ç»„åªæ˜¾ç¤ºå‰3ä¸ªé‡å¤é¡¹
                dup_title = dup['item'].get('title', 'No title')[:30] + '...'
                report_lines.append(f"    é‡å¤é¡¹: {dup_title} ({dup['reason']})")
            
            if len(group['duplicates']) > 3:
                report_lines.append(f"    ... è¿˜æœ‰ {len(group['duplicates']) - 3} ä¸ªé‡å¤é¡¹")
            report_lines.append("")
        
        if len(detection_result['duplicate_groups']) > 10:
            report_lines.append(f"... è¿˜æœ‰ {len(detection_result['duplicate_groups']) - 10} ä¸ªé‡å¤ç»„")
        
        report_lines.extend([
            "=" * 60,
            f"æ£€æµ‹æ—¶é—´: {detection_result['detection_time']}",
            "=" * 60
        ])
        
        return "\n".join(report_lines)


class DatabaseAutoDeduplicator(AutoDeduplicator):
    """
    æ•°æ®åº“è‡ªåŠ¨å»é‡å™¨
    ç›´æ¥å¯¹æ•°æ®åº“ä¸­çš„æ•°æ®è¿›è¡Œå»é‡å¤„ç†
    """
    
    def get_all_records_from_db(self) -> List[Dict[str, Any]]:
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰è®°å½•"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥æŒ‰åˆ—åè®¿é—®
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT * FROM sentiment_results 
                    ORDER BY analysis_time DESC
                ''')
                
                records = cursor.fetchall()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                data = []
                for row in records:
                    data.append(dict(row))
                
                logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å–äº† {len(data)} æ¡è®°å½•")
                return data
                
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“è·å–è®°å½•å¤±è´¥: {str(e)}")
            return []
    
    def auto_deduplicate_database(self) -> Dict[str, Any]:
        """
        è‡ªåŠ¨å¯¹æ•°æ®åº“è¿›è¡Œå»é‡å¤„ç†
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        logger.info("ğŸ—„ï¸ å¼€å§‹æ•°æ®åº“è‡ªåŠ¨å»é‡å¤„ç†...")
        
        # è·å–æ‰€æœ‰è®°å½•
        all_records = self.get_all_records_from_db()
        
        if not all_records:
            return {
                'success': False,
                'message': 'æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•',
                'stats': {}
            }
        
        # æ‰§è¡Œå»é‡
        dedup_result = self.auto_deduplicate_export_data(all_records)
        
        if not dedup_result['success']:
            return dedup_result
        
        # å¯é€‰ï¼šå°†å»é‡ç»“æœä¿å­˜å›æ•°æ®åº“æˆ–å¯¼å‡º
        # è¿™é‡Œæˆ‘ä»¬åªè¿”å›ç»“æœï¼Œä¸ä¿®æ”¹åŸæ•°æ®åº“
        
        return {
            'success': True,
            'message': f'æ•°æ®åº“å»é‡å®Œæˆ: {dedup_result["stats"]["original_count"]} â†’ {dedup_result["stats"]["final_count"]} æ¡',
            'data': dedup_result['data'],
            'stats': dedup_result['stats']
        }


# å…¨å±€å®ä¾‹
auto_deduplicator = AutoDeduplicator()
database_auto_deduplicator = DatabaseAutoDeduplicator()

def get_auto_deduplicator() -> AutoDeduplicator:
    """è·å–è‡ªåŠ¨å»é‡å™¨å®ä¾‹"""
    return auto_deduplicator

def get_database_auto_deduplicator() -> DatabaseAutoDeduplicator:
    """è·å–æ•°æ®åº“è‡ªåŠ¨å»é‡å™¨å®ä¾‹"""
    return database_auto_deduplicator

