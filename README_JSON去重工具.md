# JSON文件去重工具

这是一个专门用于处理JSON文件中重复数据的工具，以`original_id`字段为唯一标识进行去重，并支持自动导入数据库。

## 🚀 功能特点

- **智能去重**：基于`original_id`字段自动识别和删除重复记录
- **保留策略**：当发现重复记录时，保留ID值最小的记录
- **自动命名**：输出文件自动添加时间戳，避免覆盖原文件
- **详细统计**：显示去重前后的记录数、删除数量、去重率等统计信息
- **结果验证**：自动验证去重结果，确保没有残留的重复数据
- **数据库导入**：自动将去重后的结果导入到分析结果数据库中
- **重复检查**：导入前检查数据库中是否已存在相同`original_id`的记录

## 📁 文件说明

1. **`deduplicate_any_json.py`** - 核心Python脚本（支持数据库导入）
2. **`去重JSON文件.bat`** - Windows批处理脚本（双击运行，支持数据库导入选项）
3. **`去重JSON文件.ps1`** - PowerShell脚本（右键选择"使用PowerShell运行"，支持数据库导入选项）
4. **`README_JSON去重工具.md`** - 详细使用说明

## 🎯 使用方法

### 方法1：命令行使用（推荐）

```bash
# 基本用法（自动导入数据库）
python deduplicate_any_json.py <输入文件路径>

# 指定输出文件（自动导入数据库）
python deduplicate_any_json.py <输入文件路径> <输出文件路径>

# 禁用数据库导入
python deduplicate_any_json.py <输入文件路径> <输出文件路径> false

# 示例
python deduplicate_any_json.py "C:\Users\anyut\Downloads\data.json"
python deduplicate_any_json.py "data.json" "output.json" false
```

### 方法2：双击批处理文件

1. 双击 `去重JSON文件.bat`
2. 输入JSON文件路径或拖拽文件到窗口
3. 选择是否自动导入数据库（1=是，2=否）
4. 按回车键执行去重

### 方法3：PowerShell脚本

1. 右键点击 `去重JSON文件.ps1`
2. 选择"使用PowerShell运行"
3. 在文件选择对话框中选择要处理的JSON文件
4. 选择是否自动导入数据库

## 📊 输入文件要求

- 文件格式：JSON
- 数据结构：包含对象数组的JSON文件
- 必需字段：每个记录必须包含`original_id`字段
- 编码格式：UTF-8

## 📈 输出结果

- **去重后的JSON文件**：文件名格式为`原文件名_deduplicated_时间戳.json`
- **详细统计信息**：显示去重前后的数据变化
- **验证结果**：确认去重操作是否成功
- **数据库导入结果**：显示导入成功和跳过的记录数量

## 🔍 去重逻辑

1. 读取JSON文件中的所有记录
2. 以`original_id`为键，建立唯一记录映射
3. 当发现重复的`original_id`时：
   - 比较记录的`id`字段
   - 保留`id`值较小的记录
   - 删除其他重复记录
4. 输出去重后的数据到新文件
5. **自动导入数据库**（如果启用）：
   - 检查数据库中是否已存在相同`original_id`的记录
   - 跳过已存在的记录，避免重复导入
   - 将新记录保存到数据库

## 💾 数据库导入功能

### 导入的字段

工具会自动将以下字段导入到数据库：

- **基本字段**：`original_id`, `title`, `content`, `summary`, `source`, `publish_time`
- **情感分析**：`sentiment_level`, `sentiment_reason`
- **标签结果**：`tag_results`（包含14个标签的匹配结果和原因）
- **其他字段**：`companies`, `duplicate_id`, `duplication_rate`, `processing_time`

### 重复检查机制

- 基于`original_id`字段检查重复
- 如果数据库中已存在相同`original_id`的记录，则跳过导入
- 确保数据的一致性和完整性

### 数据库路径

默认数据库路径：`data/analysis_results.db`

## ⚠️ 注意事项

- 原文件不会被修改，去重后的数据保存在新文件中
- 确保JSON文件格式正确，包含有效的`original_id`字段
- 建议在处理重要数据前先备份原文件
- 如果记录缺少`original_id`字段，该记录将被跳过
- **数据库导入**：确保数据库文件存在且有写入权限
- **重复导入**：工具会自动检查并跳过已存在的记录

## 🛠️ 系统要求

- Python 3.6+
- Windows操作系统（批处理和PowerShell脚本）
- 足够的磁盘空间存储输出文件
- 数据库文件访问权限

## 📝 示例输出

```
🚀 JSON文件去重工具
==================================================
输入文件: C:\Users\anyut\Downloads\data.json
自动导入数据库: 是
📁 正在读取文件: C:\Users\anyut\Downloads\data.json
📊 原始数据记录数: 100
✅ 去重后记录数: 85
🗑️  删除重复记录数: 15
💾 去重后的数据已保存到: C:\Users\anyut\Downloads\data_deduplicated_20250826_114401.json

🔄 正在导入数据库...
📊 开始导入 85 条记录到数据库...
✅ 已导入 10/85 条记录
✅ 已导入 20/85 条记录
...
✅ 数据库导入完成！成功: 83, 跳过: 2
✅ 验证通过：没有重复的original_id

==================================================
📈 去重统计
==================================================
原始记录数: 100
去重后记录数: 85
删除重复记录数: 15
去重率: 15.00%

📋 前5条记录信息:
1. ID: 1, Original ID: 1001, Title: 示例标题1...
2. ID: 2, Original ID: 1002, Title: 示例标题2...

🎉 去重完成！
输出文件: C:\Users\anyut\Downloads\data_deduplicated_20250826_114401.json

🔍 正在验证去重结果...
✅ 验证通过：没有重复的original_id
```

## 🆘 常见问题

**Q: 为什么有些记录被跳过了？**
A: 如果记录缺少`original_id`字段，该记录会被跳过。请检查数据完整性。

**Q: 输出文件在哪里？**
A: 输出文件默认保存在输入文件的同一目录下，文件名包含时间戳。

**Q: 可以处理多大的文件？**
A: 理论上可以处理任意大小的文件，但建议单个文件不超过100MB以确保处理效率。

**Q: 支持哪些JSON格式？**
A: 支持标准的JSON格式，要求根节点是一个数组，包含多个对象记录。

**Q: 数据库导入失败怎么办？**
A: 检查数据库文件是否存在、是否有写入权限，以及数据库表结构是否正确。

**Q: 如何避免重复导入数据？**
A: 工具会自动检查`original_id`，如果数据库中已存在相同记录，会自动跳过。

## 📞 技术支持

如果遇到问题或需要功能改进，请检查：
1. Python环境是否正确安装
2. JSON文件格式是否符合要求
3. 文件路径是否正确
4. 是否有足够的磁盘空间
5. 数据库文件是否存在且有访问权限
6. 数据库表结构是否正确
