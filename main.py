"""
èˆ†æƒ…åˆ†æç³»ç»Ÿä¸»åº”ç”¨
åŸºäºFastAPIæ„å»ºçš„Web APIæœåŠ¡
"""

from fastapi import FastAPI, Request, UploadFile, File, HTTPException, Query, Form
from typing import Optional
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
import uvicorn
import csv
import io
import time
from models import AnalysisRequest, CompanyInfo, TagResult, SentimentResult
from config import Config
from api_key_manager import api_key_manager, ensure_api_key_configured
import getpass
from agents.company_agent import CompanyAgent
from agents.tag_agents import TagAgents
from agents.sentiment_agent import SentimentAgent
from fastapi.responses import StreamingResponse
from database_api import router as database_router
import pandas as pd
from datetime import datetime
from database_config_api import router as database_config_router
from analysis_api import router as analysis_router
from data_api import router as data_router
from results_api import router as results_router
from api_config_routes import router as api_config_router
from chat_api import router as chat_router
import asyncio
import json
import logging

logger = logging.getLogger(__name__)

app = FastAPI(title="å¤šAgentæƒ…æ„Ÿåˆ†æç³»ç»Ÿ", description="ä¸“é—¨å¤„ç†ä¼ä¸šè¯†åˆ«ã€æ ‡ç­¾åˆ†ç±»å’Œæƒ…æ„Ÿç­‰çº§åˆ†ç±»")

# åˆå§‹åŒ–å„ä¸ªagent
company_agent = CompanyAgent()  # ä¼ä¸šè¯†åˆ«æ¨¡å—
tag_agents = TagAgents()
sentiment_agent = SentimentAgent()
# system_agent = SystemAgent() # æš‚æ—¶æ³¨é‡Šæ‰ç³»ç»Ÿé£é™©åˆ†ææ¨¡å—

# è®¾ç½®æ¨¡æ¿å’Œé™æ€æ–‡ä»¶
templates = Jinja2Templates(directory="templates")

# è‡ªå®šä¹‰é™æ€æ–‡ä»¶å¤„ç†ï¼Œæ·»åŠ é˜²ç¼“å­˜å¤´éƒ¨
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import os

class NoCacheStaticFiles(StaticFiles):
    def file_response(self, full_path, stat_result, scope, status_code=200):
        response = FileResponse(
            full_path, stat_result=stat_result, status_code=status_code
        )
        # æ·»åŠ é˜²ç¼“å­˜å¤´éƒ¨
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
        return response

app.mount("/static", NoCacheStaticFiles(directory="static"), name="static")

# é›†æˆæ•°æ®åº“API
app.include_router(database_router)
app.include_router(database_config_router)
app.include_router(analysis_router)
app.include_router(data_router, prefix="/api/data")
app.include_router(results_router, prefix="/api/results")
app.include_router(api_config_router)
app.include_router(chat_router)

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """ä¸»é¡µ - æ˜¾ç¤ºåˆ†æç•Œé¢"""
    response = templates.TemplateResponse("index.html", {"request": request})
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response

@app.get("/parsing", response_class=HTMLResponse)
async def parsing_tasks_page(request: Request):
    """æ‰§è¡Œè§£æä»»åŠ¡é¡µé¢"""
    return templates.TemplateResponse("parsing_tasks.html", {"request": request})

# ç§»é™¤ /results è·¯ç”±ï¼Œä¸»é¡µç›´æ¥æ˜¾ç¤ºåˆ†æç»“æœ

@app.get("/config", response_class=HTMLResponse)
async def config_page(request: Request):
    """é…ç½®é¡µé¢"""
    return templates.TemplateResponse("config.html", {"request": request})

@app.get("/database-management", response_class=HTMLResponse)
async def database_management_page(request: Request):
    """æ•°æ®åº“ç®¡ç†é¡µé¢"""
    return templates.TemplateResponse("database_management.html", {"request": request})

@app.get("/database", response_class=HTMLResponse)
async def database_page(request: Request):
    """æ•°æ®åº“ç®¡ç†é¡µé¢"""
    return templates.TemplateResponse("database.html", {"request": request})

@app.get("/detail", response_class=HTMLResponse)
async def detail_page(request: Request):
    """æ–‡ç« è¯¦æƒ…é¡µé¢"""
    return templates.TemplateResponse("detail.html", {"request": request})

@app.post("/api/analyze")
async def analyze_text(request: AnalysisRequest):
    """åˆ†ææ–‡æœ¬å†…å®¹"""
    try:
        # éªŒè¯è¾“å…¥
        if not request.content or not request.content.strip():
            raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # åˆ›å»ºæµå¼å“åº”
        async def generate_stream():
            # å¼€å§‹åˆ†æ
            yield f"data: {json.dumps({'type': 'start', 'message': 'å¼€å§‹åˆ†ææ–‡æœ¬...'})}\n\n"
            
            # å®šä¹‰å¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°
            async def sentiment_analysis():
                yield f"data: {json.dumps({'type': 'progress', 'step': 'sentiment', 'message': 'æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...'})}\n\n"
                result = await sentiment_agent.analyze_sentiment(request.content)
                yield f"data: {json.dumps({'type': 'result', 'step': 'sentiment', 'data': result.model_dump()})}\n\n"

            async def tag_analysis():
                yield f"data: {json.dumps({'type': 'progress', 'step': 'tags', 'message': 'æ­£åœ¨è¿›è¡Œæ ‡ç­¾åˆ†æ...'})}\n\n"
                results = await tag_agents.analyze_tags(request.content)
                for tag_result in results:
                    yield f"data: {json.dumps({'type': 'progress', 'step': 'tags', 'message': f'æ­£åœ¨åˆ†ææ ‡ç­¾: {tag_result.tag}'})}\n\n"
                    yield f"data: {json.dumps({'type': 'result', 'step': 'tags', 'data': tag_result.model_dump()})}\n\n"
                    tag_status = 'åŒ¹é…' if getattr(tag_result, 'belongs', False) else 'ä¸åŒ¹é…'
                    yield f"data: {json.dumps({'type': 'progress', 'step': 'tags', 'message': f'æ ‡ç­¾ {tag_result.tag} åˆ†æå®Œæˆ: {tag_status}'})}\n\n"

            async def company_analysis():
                yield f"data: {json.dumps({'type': 'progress', 'step': 'companies', 'message': 'æ­£åœ¨è¯†åˆ«ä¼ä¸šä¿¡æ¯...'})}\n\n"
                results = await company_agent.analyze_companies(request.content)
                # ç°åœ¨ä¼ä¸šè¯†åˆ«åªè¿”å›ä¼ä¸šåç§°ï¼Œè½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
                company_data = [{"name": c.name, "credit_code": "", "reason": f"LLMæ™ºèƒ½è¯†åˆ«: {c.name}"} for c in results]
                yield f"data: {json.dumps({'type': 'result', 'step': 'companies', 'data': company_data})}\n\n"

            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            generators = [
                sentiment_analysis(),
                tag_analysis(),
                company_analysis()
            ]
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            for generator in generators:
                async for message in generator:
                    yield message
            

            
            # å®Œæˆåˆ†æ
            yield f"data: {json.dumps({'type': 'complete', 'message': 'åˆ†æå®Œæˆï¼'})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "å¤šAgentæƒ…æ„Ÿåˆ†æç³»ç»Ÿè¿è¡Œæ­£å¸¸"}

@app.post("/api/upload_csv")
async def upload_csv(file: UploadFile = File(...)):
    """ä¸Šä¼ CSVæ–‡ä»¶è¿›è¡Œæ‰¹é‡åˆ†æ"""
    if not file.filename.endswith('.csv'):
        return {"detail": "åªæ”¯æŒCSVæ–‡ä»¶"}
    
    try:
        # è¯»å–CSVæ–‡ä»¶å†…å®¹
        content = await file.read()
        content_str = content.decode('utf-8')
        
        # è§£æCSV
        csv_reader = csv.DictReader(io.StringIO(content_str))
        rows = list(csv_reader)
        
        if not rows:
            return {"detail": "CSVæ–‡ä»¶ä¸ºç©º"}
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        if 'content' not in rows[0]:
            return {"detail": "CSVæ–‡ä»¶å¿…é¡»åŒ…å«'content'åˆ—"}
        
        # æ‰¹é‡åˆ†æ
        results = []
        for row in rows:
            content_text = row['content'].strip()
            if content_text:
                # åˆ†æå•æ¡å†…å®¹
                # åˆ›å»ºæ‰€æœ‰åˆ†æä»»åŠ¡
                tasks = [
                    company_agent.analyze_companies(content_text),
                    tag_agents.analyze_tags(content_text),
                    sentiment_agent.analyze_sentiment(content_text)
                ]
                
                # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
                companies, tags, sentiment = await asyncio.gather(*tasks)
                
                # ç›´æ¥ä½¿ç”¨å¯¹è±¡çš„model_dumpæ–¹æ³•è½¬æ¢ä¸ºå­—å…¸
                results.append({
                    "content": content_text,
                    "companies": [{"name": company.name, "credit_code": "", "reason": f"LLMæ™ºèƒ½è¯†åˆ«: {company.name}"} for company in companies],
                    "tags": [tag.model_dump() for tag in tags],
                    "sentiment": sentiment.model_dump()
                })
        
        return {
            "total": len(results),
            "results": results
        }
        
    except Exception as e:
        return {"detail": f"å¤„ç†CSVæ–‡ä»¶å¤±è´¥: {str(e)}"}

@app.post("/api/batch_parse")
async def batch_parse_data(request: Request):
    """æ‰¹é‡è§£ææ•°æ®æ¥å£ - ä½¿ç”¨çœŸå®æ•°æ®"""
    try:
        from database_manager import UnifiedDatabaseManager
        from result_database_new import ResultDatabase
        import hashlib
        
        body = await request.json()
        data_source = body.get("data_source", "èˆ†æƒ…æ•°æ®")
        data_range = body.get("data_range", "all")
        # å…¼å®¹ä¸¤ç§å‚æ•°åç§°ï¼šä¼˜å…ˆä½¿ç”¨ start_time/end_timeï¼Œå›é€€åˆ° start_date/end_date
        start_date = body.get("start_time") or body.get("start_date")
        end_date = body.get("end_time") or body.get("end_date")
        
        enable_sentiment = body.get("enable_sentiment", True)
        enable_tags = body.get("enable_tags", True)
        enable_companies = body.get("enable_companies", True)
        
        # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID
        import uuid
        session_id = str(uuid.uuid4())
        
        # åˆ›å»ºæµå¼å“åº”
        async def generate_stream():
            # è°ƒè¯•ä¿¡æ¯
            start_time_param = body.get("start_time")
            end_time_param = body.get("end_time")
            yield f"data: {json.dumps({'type': 'log', 'message': f'æ¥æ”¶åˆ°çš„å‚æ•°: start_time={start_time_param}, end_time={end_time_param}'})}\n\n"
            yield f"data: {json.dumps({'type': 'log', 'message': f'è§£æåçš„æ—¶é—´: start_date={start_date}, end_date={end_date}'})}\n\n"
            try:
                yield f"data: {json.dumps({'type': 'start', 'message': f'å¼€å§‹æ‰¹é‡è§£æ {data_source} æ•°æ®...'})}\n\n"
                
                # è·å–æ•°æ®åº“ç®¡ç†å™¨
                db_manager = UnifiedDatabaseManager()
                result_db = ResultDatabase('data/analysis_results.db')
                
                # åˆå§‹åŒ–é‡å¤æ£€æµ‹ç®¡ç†å™¨
                from text_deduplicator import DuplicateDetectionManager
                duplicate_manager = DuplicateDetectionManager({
                    'similarity_threshold': 0.6,  # é™ä½é˜ˆå€¼ä»¥æ•è·æ›´å¤šç›¸ä¼¼æ–‡æœ¬
                    'hamming_threshold': 25        # åŸºäºæµ‹è¯•ç»“æœï¼Œæ±‰æ˜è·ç¦»25å¯ä»¥æ•è·ç›¸ä¼¼æ–‡æœ¬
                })
                
                yield f"data: {json.dumps({'type': 'log', 'message': 'åˆå§‹åŒ–SimHashé‡å¤æ£€æµ‹ç³»ç»Ÿ...'})}\n\n"
                
                # ä»èˆ†æƒ…æ•°æ®åº“è·å–å®é™…æ•°æ®
                sentiment_db = db_manager.get_sentiment_database()
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶ - ä¿®å¤SQLiteå‚æ•°ç»‘å®šé—®é¢˜
                filters = {}
                if start_date and end_date:
                    # ä½¿ç”¨ä¸ç­›é€‰æ•°æ®é‡ç›¸åŒçš„æ—¶é—´èŒƒå›´å¤„ç†æ–¹å¼
                    # ç¡®ä¿æ—¶é—´æ ¼å¼ç»Ÿä¸€ï¼Œç²¾ç¡®åˆ°åˆ†é’Ÿ
                    filters["publish_time"] = {
                        "start": start_date,
                        "end": end_date
                    }
                    
                    # è®°å½•æ—¶é—´èŒƒå›´ä¿¡æ¯
                    yield f"data: {json.dumps({'type': 'log', 'message': f'æŸ¥è¯¢æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}'})}\n\n"
                else:
                    yield f"data: {json.dumps({'type': 'log', 'message': 'æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œå°†æŸ¥è¯¢æ‰€æœ‰æ•°æ®'})}\n\n"
                
                # å…ˆè·å–æ•°æ®æ€»é‡ - ä½¿ç”¨ä¸ç­›é€‰æ•°æ®é‡ç›¸åŒçš„æŸ¥è¯¢æ–¹å¼
                count_result = sentiment_db.get_data_count(filters=filters)
                
                if not count_result['success']:
                    error_msg = count_result.get('message', 'æœªçŸ¥é”™è¯¯')
                    yield f"data: {json.dumps({'type': 'error', 'message': f'è·å–æ•°æ®é‡å¤±è´¥: {error_msg}'})}\n\n"
                    return
                
                total_available = count_result['total']
                
                if total_available == 0:
                    yield f"data: {json.dumps({'type': 'complete', 'total_processed': 0, 'message': 'æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ†æçš„æ•°æ®'})}\n\n"
                    return
                
                yield f"data: {json.dumps({'type': 'log', 'message': f'æ‰¾åˆ° {total_available} æ¡æ•°æ®éœ€è¦åˆ†æ'})}\n\n"
                
                # æŸ¥è¯¢æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼ˆä¸è®¾ç½®æ•°é‡é™åˆ¶ï¼‰
                data_result = sentiment_db.get_data(
                    fields=["*"],
                    filters=filters,
                    page=1,
                    page_size=total_available  # ä½¿ç”¨å®é™…çš„æ•°æ®æ€»é‡
                )
                
                if not data_result['success']:
                    error_msg = data_result.get('message', 'æœªçŸ¥é”™è¯¯')
                    yield f"data: {json.dumps({'type': 'error', 'message': f'è·å–æ•°æ®å¤±è´¥: {error_msg}'})}\n\n"
                    return
                
                source_data = data_result['data']
                total_items = len(source_data)
                
                processed = 0
                success_count = 0
                failed_count = 0
                
                # æ”¶é›†æ‰€æœ‰æ•°æ®ç”¨äºæ‰¹é‡é‡å¤æ£€æµ‹
                all_data_items = []
                
                # å¤„ç†æ¯æ¡æ•°æ®
                for i, data_item in enumerate(source_data):
                    try:
                        processed += 1
                        processing_start_time = time.time()  # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
                        progress = (processed / total_items) * 100
                        
                        yield f"data: {json.dumps({'type': 'progress', 'current': processed, 'total': total_items, 'percentage': progress})}\n\n"
                        
                        # æå–æ–‡æœ¬å†…å®¹
                        content_text = data_item.get('content', '') or data_item.get('title', '')
                        if not content_text:
                            yield f"data: {json.dumps({'type': 'log', 'message': f'ç¬¬ {processed} æ¡æ•°æ®å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡'})}\n\n"
                            failed_count += 1
                            continue
                        
                        content_preview = content_text[:50] + "..." if len(content_text) > 50 else content_text
                        yield f"data: {json.dumps({'type': 'log', 'message': f'æ­£åœ¨åˆ†æç¬¬ {processed} æ¡æ•°æ®: {content_preview}'})}\n\n"
                        
                        # ç”ŸæˆåŸå§‹ID (ä½¿ç”¨åºå·)
                        original_id = processed
                        
                        # åˆ›å»ºåˆ†æä»»åŠ¡
                        analysis_tasks = []
                        if enable_sentiment:
                            analysis_tasks.append(sentiment_agent.analyze_sentiment(content_text))
                        if enable_tags:
                            analysis_tasks.append(tag_agents.analyze_tags(content_text))
                        if enable_companies:
                            analysis_tasks.append(company_agent.analyze_companies(content_text))
                        
                        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
                        analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
                        
                        # è§£æåˆ†æç»“æœ
                        sentiment_result = None
                        tag_results = []
                        company_results = []
                        
                        result_index = 0
                        if enable_sentiment:
                            sentiment_result = analysis_results[result_index] if not isinstance(analysis_results[result_index], Exception) else None
                            result_index += 1
                        if enable_tags:
                            tag_results = analysis_results[result_index] if not isinstance(analysis_results[result_index], Exception) else []
                            result_index += 1
                        if enable_companies:
                            company_results = analysis_results[result_index] if not isinstance(analysis_results[result_index], Exception) else []
                        
                        # æ„å»ºæ ‡ç­¾ç»“æœå­—å…¸
                        tag_results_dict = {}
                        if tag_results:
                            for tag_result in tag_results:
                                tag_results_dict[tag_result.tag] = {
                                    'belongs': tag_result.belongs,
                                    'reason': tag_result.reason
                                }
                        
                        # ç”Ÿæˆå†…å®¹æ‘˜è¦ - ä½¿ç”¨AIç”ŸæˆçœŸæ­£çš„æ‘˜è¦
                        try:
                            from ali_llm_client import AliLLMClient
                            llm_client = AliLLMClient()
                            summary = llm_client.generate_summary(content_text)
                            yield f"data: {json.dumps({'type': 'log', 'message': f'ç¬¬ {processed} æ¡æ•°æ®æ‘˜è¦ç”Ÿæˆå®Œæˆ'})}\n\n"
                        except Exception as e:
                            summary = content_text[:200] + "..." if len(content_text) > 200 else content_text
                            yield f"data: {json.dumps({'type': 'warning', 'message': f'ç¬¬ {processed} æ¡æ•°æ®æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æˆªå–æ‘˜è¦: {str(e)}'})}\n\n"
                        
                        # å‡†å¤‡æ•°æ®ç”¨äºé‡å¤æ£€æµ‹
                        data_for_duplicate = {
                            'id': original_id,
                            'content': content_text,
                            'title': data_item.get('title', 'æ— æ ‡é¢˜'),
                            'publish_time': data_item.get('publish_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
                        }
                        all_data_items.append(data_for_duplicate)
                        
                        # æš‚æ—¶å­˜å‚¨åˆ†æç»“æœï¼Œç­‰å¾…é‡å¤æ£€æµ‹
                        temp_save_data = {
                            'original_id': original_id,
                            'title': data_item.get('title', 'æ— æ ‡é¢˜'),
                            'content': content_text,
                            'summary': summary,
                            'source': data_item.get('source', data_source),
                            'publish_time': data_item.get('publish_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                            'sentiment_level': sentiment_result.level if sentiment_result else 'æœªçŸ¥',
                            'sentiment_reason': sentiment_result.reason if sentiment_result else 'æ— åŸå› ',
                            'companies': ','.join([company.name for company in company_results]) if company_results else '',
                            'processing_time': round(time.time() - processing_start_time, 2),  # å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
                            'tag_results': tag_results_dict
                        }
                        
                        # å°†åˆ†æç»“æœä¸æ•°æ®é¡¹å…³è”
                        data_for_duplicate['analysis_result'] = temp_save_data
                        
                        yield f"data: {json.dumps({'type': 'log', 'message': f'ç¬¬ {processed} æ¡æ•°æ®åˆ†æå®Œæˆï¼Œç­‰å¾…é‡å¤æ£€æµ‹...'})}\n\n"
                        
                    except Exception as e:
                        failed_count += 1
                        yield f"data: {json.dumps({'type': 'log', 'message': f'ç¬¬ {processed} æ¡æ•°æ®åˆ†æå¤±è´¥: {str(e)}'})}\n\n"
                        continue
                
                # æ‰§è¡Œæ‰¹é‡é‡å¤æ£€æµ‹
                if all_data_items:
                    yield f"data: {json.dumps({'type': 'log', 'message': f'å¼€å§‹æ‰§è¡ŒSimHashé‡å¤æ£€æµ‹ï¼Œå…± {len(all_data_items)} æ¡æ•°æ®...'})}\n\n"
                    
                    try:
                        # æ‰§è¡Œé‡å¤æ£€æµ‹
                        duplicated_results = duplicate_manager.detect_duplicates(all_data_items)
                        
                        yield f"data: {json.dumps({'type': 'log', 'message': 'é‡å¤æ£€æµ‹å®Œæˆï¼Œå¼€å§‹ä¿å­˜åˆ°æ•°æ®åº“...'})}\n\n"
                        
                        # ä¿å­˜å¸¦æœ‰é‡å¤æ£€æµ‹ç»“æœçš„æ•°æ®
                        for result_item in duplicated_results:
                            try:
                                analysis_result = result_item['analysis_result']
                                
                                # æ·»åŠ é‡å¤æ£€æµ‹ç»“æœ
                                save_data = {
                                    **analysis_result,
                                    'duplicate_id': result_item['duplicate_id'],
                                    'duplication_rate': result_item['duplication_rate'],
                                    'session_id': session_id,  # æ·»åŠ ä¼šè¯ID
                                }
                                
                                # ä¿å­˜åˆ°ç»“æœæ•°æ®åº“
                                save_result = result_db.save_analysis_result(save_data)
                                
                                if save_result['success']:
                                    success_count += 1
                                elif save_result.get('duplicate', False):
                                    # è·³è¿‡é‡å¤è®°å½•ï¼Œä¸ç®—ä½œå¤±è´¥
                                    item_id = result_item['id']
                                    yield f"data: {json.dumps({'type': 'log', 'message': f'ID {item_id} å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤ä¿å­˜'})}\n\n"
                                else:
                                    save_error = save_result.get('message', 'æœªçŸ¥é”™è¯¯')
                                    failed_count += 1
                                    item_id = result_item['id']
                                    yield f"data: {json.dumps({'type': 'log', 'message': f'ID {item_id} ä¿å­˜å¤±è´¥: {save_error}'})}\n\n"
                                    
                            except Exception as e:
                                failed_count += 1
                                item_id = result_item.get('id', 'æœªçŸ¥')
                                yield f"data: {json.dumps({'type': 'log', 'message': f'ID {item_id} å¤„ç†å¤±è´¥: {str(e)}'})}\n\n"
                        
                        # è¾“å‡ºé‡å¤æ£€æµ‹ç»Ÿè®¡
                        duplicate_count = sum(1 for item in duplicated_results if item['is_duplicate'])
                        yield f"data: {json.dumps({'type': 'log', 'message': f'é‡å¤æ£€æµ‹å®Œæˆï¼å‘ç° {duplicate_count} æ¡é‡å¤æ–‡æœ¬'})}\n\n"
                        
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'error', 'message': f'é‡å¤æ£€æµ‹å¤±è´¥: {str(e)}'})}\n\n"
                
                # å®Œæˆ
                completion_msg = f'æ‰¹é‡è§£æå®Œæˆï¼æ€»å¤„ç†: {processed}, æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}'
                yield f"data: {json.dumps({'type': 'complete', 'total_processed': processed, 'success_count': success_count, 'failed_count': failed_count, 'session_id': session_id, 'message': completion_msg})}\n\n"
                
                # è‡ªåŠ¨è§¦å‘å»é‡å’Œå¯¼å‡ºæµç¨‹
                if success_count > 0:
                    yield f"data: {json.dumps({'type': 'log', 'message': 'å¼€å§‹è‡ªåŠ¨å»é‡å’Œå¯¼å‡ºæµç¨‹...'})}\n\n"
                    
                    try:
                        # æ‰§è¡Œè‡ªåŠ¨å»é‡
                        yield f"data: {json.dumps({'type': 'log', 'message': 'æ­£åœ¨æ‰§è¡Œæ•°æ®åº“å»é‡...'})}\n\n"
                        
                        # è°ƒç”¨å»é‡æ¨¡å—
                        from deduplicate_any_json import deduplicate_database_records
                        dedup_result = deduplicate_database_records()
                        
                        if dedup_result['success']:
                            yield f"data: {json.dumps({'type': 'log', 'message': f'å»é‡å®Œæˆï¼åˆ é™¤é‡å¤è®°å½•: {dedup_result["duplicates_removed"]}'})}\n\n"
                            
                            # æ‰§è¡Œè‡ªåŠ¨å¯¼å‡º
                            yield f"data: {json.dumps({'type': 'log', 'message': 'æ­£åœ¨æ‰§è¡Œè‡ªåŠ¨å¯¼å‡º...'})}\n\n"
                            
                            from deduplicate_any_json import auto_export_after_dedup
                            export_result = auto_export_after_dedup()
                            
                            if export_result['success']:
                                yield f"data: {json.dumps({'type': 'log', 'message': f'è‡ªåŠ¨å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶: {export_result["export_file"]}'})}\n\n"
                            else:
                                yield f"data: {json.dumps({'type': 'warning', 'message': f'è‡ªåŠ¨å¯¼å‡ºå¤±è´¥: {export_result["message"]}'})}\n\n"
                        else:
                            yield f"data: {json.dumps({'type': 'warning', 'message': f'å»é‡å¤±è´¥: {dedup_result["message"]}'})}\n\n"
                            
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'warning', 'message': f'è‡ªåŠ¨å»é‡å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'})}\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'message': f'æ‰¹é‡è§£æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡è§£æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡è§£æå¤±è´¥: {str(e)}")

@app.get("/api/analysis_results")
async def get_analysis_results(
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
    data_source: Optional[str] = Query(None, description="æ•°æ®æº"),
    sentiment: Optional[str] = Query(None, description="æƒ…æ„Ÿç­‰çº§"),
    tags: Optional[str] = Query(None, description="é£é™©æ ‡ç­¾"),
    date_range: Optional[str] = Query(None, description="æ—¶é—´èŒƒå›´"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(10, ge=1, le=100, description="æ¯é¡µå¤§å°")
):
    """è·å–åˆ†æç»“æœæ•°æ®"""
    try:
        from result_database_new import ResultDatabase
        
        # ç›´æ¥ä½¿ç”¨ç»“æœæ•°æ®åº“ - ä¿®å¤æ•°æ®åº“è·¯å¾„
        result_db = ResultDatabase('data/analysis_results.db')
        
        # è·å–åˆ†æç»“æœï¼Œæ”¯æŒæœç´¢
        result = result_db.get_analysis_results(
            page=page, 
            page_size=page_size,
            search_keyword=search
        )
        
        logger.info(f"æ•°æ®åº“æŸ¥è¯¢ç»“æœ - success: {result.get('success')}, total: {result.get('total')}")
        
        if result['success']:
            # ResultDatabaseå·²ç»è¿”å›äº†æ­£ç¡®æ ¼å¼çš„æ•°æ®ï¼Œç›´æ¥è¿”å›
            # ç¡®ä¿è¿”å›JSONResponseä»¥æ­£ç¡®è®¾ç½®Content-Typeå’Œç¼–ç 
            from fastapi.responses import JSONResponse
            return JSONResponse(
                content=result,
                media_type="application/json; charset=utf-8"
            )
        else:
            return {
                "success": False,
                "error": result.get('message', 'æŸ¥è¯¢å¤±è´¥'),
                "data": [],
                "total": 0,
                "page": page,
                "page_size": page_size,
                "total_pages": 0
            }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}",
            "data": [],
            "total": 0,
            "page": page,
            "page_size": page_size,
            "total_pages": 0
        }

@app.get("/api/duplicates/statistics")
async def get_duplicate_statistics():
    """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
    # è¿™é‡Œå¯ä»¥è¿æ¥åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶ç³»ç»Ÿæ¥è·å–çœŸå®çš„ç»Ÿè®¡ä¿¡æ¯
    # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
    return {
        "total_texts": 0,
        "unique_texts": 0,
        "duplicate_texts": 0,
        "duplication_rate": 0.0
    }

@app.post("/api/export/enhanced")
async def enhanced_export(
    export_format: str = Form("json", description="å¯¼å‡ºæ ¼å¼ (json/csv/excel)"),
    include_metadata: bool = Form(True, description="æ˜¯å¦åŒ…å«å…ƒæ•°æ®"),
    filter_tags: Optional[str] = Form(None, description="è¿‡æ»¤æ ‡ç­¾ï¼Œé€—å·åˆ†éš”")
):
    """å¢å¼ºæ•°æ®å¯¼å‡ºæ¥å£"""
    try:
        from deduplicate_any_json import enhanced_export_data
        
        # å¤„ç†æ ‡ç­¾è¿‡æ»¤
        tags_list = None
        if filter_tags:
            tags_list = [tag.strip() for tag in filter_tags.split(',') if tag.strip()]
        
        # æ‰§è¡Œå¢å¼ºå¯¼å‡º
        export_result = enhanced_export_data(
            export_format=export_format,
            include_metadata=include_metadata,
            filter_tags=tags_list
        )
        
        if export_result['success']:
            return {
                "success": True,
                "message": export_result['message'],
                "export_file": export_result['export_file'],
                "total_records": export_result['total_records'],
                "format": export_result['format']
            }
        else:
            raise HTTPException(status_code=400, detail=export_result['message'])
            
    except Exception as e:
        logger.error(f"å¢å¼ºå¯¼å‡ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¢å¼ºå¯¼å‡ºå¤±è´¥: {str(e)}")

@app.get("/api/config")
async def get_config():
    """è·å–å½“å‰é…ç½®ï¼ˆä»…è¿”å›éæ•æ„Ÿé¡¹å’ŒAPI Keyæ©ç ï¼‰"""
    masked_key = None
    if Config.ALI_API_KEY:
        # æ˜¾ç¤ºå4ä½
        masked_key = ("*" * max(0, len(Config.ALI_API_KEY) - 4)) + Config.ALI_API_KEY[-4:]
    return {
        "ALI_MODEL_NAME": Config.ALI_MODEL_NAME,
        "ALI_BASE_URL": Config.ALI_BASE_URL,
        "ALI_API_KEY_MASKED": masked_key,
        "HOST": Config.HOST,
        "PORT": Config.PORT,
        "DEBUG": Config.DEBUG,
        "MAX_CONTENT_LENGTH": Config.MAX_CONTENT_LENGTH,
        "BATCH_SIZE": Config.BATCH_SIZE,
        "TAG_PROMPT_TEMPLATE": Config.TAG_PROMPT_TEMPLATE,
        "AGENT_PROMPTS": Config.AGENT_PROMPTS,
    }

@app.post("/api/config")
async def update_config(payload: dict):
    """æ›´æ–°è¿è¡Œæ—¶é…ç½®ï¼ˆè¿›ç¨‹å†…ï¼Œä»…å½“æ¬¡ç”Ÿæ•ˆï¼‰ã€‚å»ºè®®æŒä¹…åŒ–åˆ°.envæ‰‹åŠ¨ç®¡ç†ã€‚"""
    # å…è®¸æ›´æ–°çš„å­—æ®µ
    updatable_fields = {
        "ALI_API_KEY": str,
        "ALI_MODEL_NAME": str,
        "ALI_BASE_URL": str,
        "HOST": str,
        "PORT": int,
        "DEBUG": bool,
        "MAX_CONTENT_LENGTH": int,
        "BATCH_SIZE": int,
        "TAG_PROMPT_TEMPLATE": str,
    }
    updated = {}
    for key, caster in updatable_fields.items():
        if key in payload and payload[key] is not None:
            try:
                value = payload[key]
                # æ‰‹åŠ¨å¤„ç†bool/intç­‰
                if caster is bool and isinstance(value, str):
                    value = value.lower() == "true"
                elif caster is int and isinstance(value, str):
                    value = int(value)
                setattr(Config, key, value)
                updated[key] = value if key != "ALI_API_KEY" else "UPDATED"
            except Exception as e:
                return JSONResponse(status_code=400, content={"detail": f"é…ç½®é¡¹ {key} æ— æ•ˆ: {e}"})

    # ç‰¹æ®Šå¤„ç†AGENT_PROMPTS
    if "AGENT_PROMPTS" in payload and payload["AGENT_PROMPTS"] is not None:
        try:
            agent_prompts = payload["AGENT_PROMPTS"]
            if isinstance(agent_prompts, dict):
                # æ›´æ–°Configä¸­çš„AGENT_PROMPTS
                Config.AGENT_PROMPTS.update(agent_prompts)

                # æ›´æ–°æ‰€æœ‰ç›¸å…³Agentçš„æç¤ºè¯
                for tag_name, prompt in agent_prompts.items():
                    if hasattr(tag_agents, 'update_agent_prompt'):
                        tag_agents.update_agent_prompt(tag_name, prompt)

                    # å•ç‹¬å¤„ç†æƒ…æ„Ÿåˆ†æAgentçš„æç¤ºè¯æ›´æ–°
                    if tag_name == "æƒ…æ„Ÿåˆ†æ":
                        sentiment_agent.prompt_template = prompt or Config.SENTIMENT_PROMPT_TEMPLATE

                updated["AGENT_PROMPTS"] = "UPDATED"
            else:
                return JSONResponse(status_code=400, content={"detail": "AGENT_PROMPTSå¿…é¡»æ˜¯å­—å…¸æ ¼å¼"})
        except Exception as e:
            return JSONResponse(status_code=400, content={"detail": f"AGENT_PROMPTSæ›´æ–°å¤±è´¥: {e}"})

    # åŒæ­¥åˆ°ç›¸å…³è¿è¡Œå®ä¾‹ï¼ˆä¾‹å¦‚AliLLMClientï¼‰ï¼Œæ­¤å¤„ä»…åœ¨ä¸‹æ¬¡å®ä¾‹åŒ–ç”Ÿæ•ˆï¼›
    # å¦‚éœ€ç«‹åˆ»ç”Ÿæ•ˆï¼Œå¯è€ƒè™‘é‡æ–°åˆ›å»ºç›¸å…³å®¢æˆ·ç«¯å®ä¾‹ã€‚
    return {"message": "é…ç½®å·²æ›´æ–°ï¼ˆè¿›ç¨‹å†…ï¼‰", "updated": updated}


def setup_api_key():
    """åœ¨å¯åŠ¨æ—¶è¦æ±‚ç”¨æˆ·è¾“å…¥APIå¯†é’¥"""
    print("=" * 60)
    print("ğŸš€ èˆ†æƒ…åˆ†æç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰APIå¯†é’¥
    try:
        if api_key_manager.get_api_key():
            print("âœ… æ£€æµ‹åˆ°å·²é…ç½®çš„APIå¯†é’¥")
            return
    except:
        pass
    
    print("\nğŸ“ è¯·é…ç½®é˜¿é‡Œäº‘APIå¯†é’¥ä»¥å¯ç”¨æ™ºèƒ½åˆ†æåŠŸèƒ½")
    print("ğŸ’¡ æ‚¨å¯ä»¥åœ¨é˜¿é‡Œäº‘æ§åˆ¶å°è·å–APIå¯†é’¥")
    print("ğŸ”— è·å–åœ°å€: https://dashscope.console.aliyun.com/")
    
    while True:
        try:
            api_key = getpass.getpass("\nè¯·è¾“å…¥æ‚¨çš„é˜¿é‡Œäº‘APIå¯†é’¥: ").strip()
            
            if not api_key:
                print("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
                
            # éªŒè¯APIå¯†é’¥æ ¼å¼ï¼ˆç®€å•éªŒè¯ï¼‰
            if len(api_key) < 20:
                print("âŒ APIå¯†é’¥æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥åé‡æ–°è¾“å…¥")
                continue
            
            # ä¿å­˜APIå¯†é’¥
            api_key_manager.set_api_key(api_key)
            
            # è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
            try:
                config = Config()
                # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–é»˜è®¤é…ç½®
                print("âœ… APIå¯†é’¥é…ç½®æˆåŠŸï¼")
                print("ğŸ’¾ é…ç½®å·²è‡ªåŠ¨ä¿å­˜åˆ°ç³»ç»Ÿ")
            except Exception as e:
                print(f"âš ï¸  APIå¯†é’¥å·²ä¿å­˜ï¼Œä½†é…ç½®ä¿å­˜å¤±è´¥: {e}")
            
            break
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆå¯åŠ¨")
            exit(0)
        except Exception as e:
            print(f"âŒ é…ç½®APIå¯†é’¥æ—¶å‡ºé”™: {e}")
            continue
    
    print("\nğŸ‰ ç³»ç»Ÿé…ç½®å®Œæˆï¼Œæ­£åœ¨å¯åŠ¨æœåŠ¡...")
    print("=" * 60)


if __name__ == "__main__":
    # å¯åŠ¨æ—¶é…ç½®APIå¯†é’¥
    setup_api_key()
    
    print("ğŸŒ æœåŠ¡å¯åŠ¨ä¸­...")
    print("ğŸ“ è®¿é—®åœ°å€: http://localhost:8000")
    print("ğŸ“Š ç®¡ç†ç•Œé¢: http://localhost:8000/config")
    print("ğŸ’¬ æ™ºèƒ½èŠå¤©: ç‚¹å‡»å³ä¸‹è§’èŠå¤©å›¾æ ‡")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000) 