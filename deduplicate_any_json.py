#!/usr/bin/env python3
"""é€šç”¨JSONæ–‡ä»¶å»é‡è„šæœ¬ - ä»¥åŸå§‹IDè¿›è¡Œå»é‡ï¼Œå¹¶è‡ªåŠ¨å¯¼å…¥æ•°æ®åº“"""

import json
import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

try:
    from result_database_new import ResultDatabase
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False
    print("âš ï¸  è­¦å‘Šï¼šæ— æ³•å¯¼å…¥æ•°æ®åº“æ¨¡å—ï¼Œå°†è·³è¿‡æ•°æ®åº“å¯¼å…¥åŠŸèƒ½")

def deduplicate_json_by_original_id(input_file_path, output_file_path=None, auto_import_db=True):
    """
    å¯¹JSONæ–‡ä»¶æŒ‰åŸå§‹IDè¿›è¡Œå»é‡ï¼Œå¹¶å¯é€‰æ‹©è‡ªåŠ¨å¯¼å…¥æ•°æ®åº“
    
    Args:
        input_file_path: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
        output_file_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        auto_import_db: æ˜¯å¦è‡ªåŠ¨å¯¼å…¥æ•°æ®åº“ï¼Œé»˜è®¤True
    
    Returns:
        str: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # è¯»å–JSONæ–‡ä»¶
        print(f"ğŸ“ æ­£åœ¨è¯»å–æ–‡ä»¶: {input_file_path}")
        with open(input_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print("âŒ é”™è¯¯ï¼šJSONæ–‡ä»¶åº”è¯¥åŒ…å«ä¸€ä¸ªæ•°ç»„")
            return None
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®è®°å½•æ•°: {len(data)}")
        
        # æŒ‰original_idè¿›è¡Œå»é‡ï¼Œä¿ç•™IDæœ€å°çš„è®°å½•
        unique_records = {}
        duplicates_count = 0
        missing_original_id_count = 0
        
        for record in data:
            if not isinstance(record, dict):
                continue
                
            original_id = record.get('original_id')
            if original_id is None:
                missing_original_id_count += 1
                continue
                
            if original_id not in unique_records:
                unique_records[original_id] = record
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ¯”è¾ƒIDï¼Œä¿ç•™IDè¾ƒå°çš„è®°å½•
                existing_id = unique_records[original_id].get('id')
                current_id = record.get('id')
                
                # å¤„ç†IDä¸ºNoneçš„æƒ…å†µ
                if existing_id is None:
                    existing_id = float('inf')
                if current_id is None:
                    current_id = float('inf')
                
                if current_id < existing_id:
                    unique_records[original_id] = record
                duplicates_count += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        deduplicated_data = list(unique_records.values())
        
        print(f"âœ… å»é‡åè®°å½•æ•°: {len(deduplicated_data)}")
        print(f"ğŸ—‘ï¸  åˆ é™¤é‡å¤è®°å½•æ•°: {duplicates_count}")
        if missing_original_id_count > 0:
            print(f"âš ï¸  ç¼ºå°‘original_idçš„è®°å½•æ•°: {missing_original_id_count}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_file_path is None:
            input_dir = os.path.dirname(input_file_path)
            input_name = os.path.splitext(os.path.basename(input_file_path))[0]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file_path = os.path.join(input_dir, f"{input_name}_deduplicated_{timestamp}.json")
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(deduplicated_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å»é‡åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file_path}")
        
        # è‡ªåŠ¨å¯¼å…¥æ•°æ®åº“
        if auto_import_db and DATABASE_AVAILABLE:
            print("\nğŸ”„ æ­£åœ¨å¯¼å…¥æ•°æ®åº“...")
            import_result = import_to_database(deduplicated_data)
            if import_result['success']:
                print(f"âœ… æ•°æ®åº“å¯¼å…¥æˆåŠŸï¼å¯¼å…¥è®°å½•æ•°: {import_result['imported_count']}")
                if import_result['skipped_count'] > 0:
                    print(f"âš ï¸  è·³è¿‡è®°å½•æ•°: {import_result['skipped_count']}")
            else:
                print(f"âŒ æ•°æ®åº“å¯¼å…¥å¤±è´¥: {import_result['message']}")
        elif auto_import_db and not DATABASE_AVAILABLE:
            print("âš ï¸  æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®åº“å¯¼å…¥")
        
        # æ˜¾ç¤ºå»é‡ç»Ÿè®¡
        print("\n" + "="*50)
        print("ğŸ“ˆ å»é‡ç»Ÿè®¡")
        print("="*50)
        print(f"åŸå§‹è®°å½•æ•°: {len(data)}")
        print(f"å»é‡åè®°å½•æ•°: {len(deduplicated_data)}")
        print(f"åˆ é™¤é‡å¤è®°å½•æ•°: {duplicates_count}")
        if len(data) > 0:
            print(f"å»é‡ç‡: {duplicates_count/len(data)*100:.2f}%")
        
        # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•çš„åŸºæœ¬ä¿¡æ¯
        if deduplicated_data:
            print(f"\nğŸ“‹ å‰{min(5, len(deduplicated_data))}æ¡è®°å½•ä¿¡æ¯:")
            for i, record in enumerate(deduplicated_data[:5], 1):
                title = record.get('title', 'æ— æ ‡é¢˜')
                if len(title) > 40:
                    title = title[:40] + "..."
                print(f"{i}. ID: {record.get('id', 'None')}, Original ID: {record.get('original_id')}, Title: {title}")
        
        return output_file_path
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {input_file_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None

def import_to_database(data_records):
    """
    å°†å»é‡åçš„æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“
    
    Args:
        data_records: æ•°æ®è®°å½•åˆ—è¡¨
    
    Returns:
        dict: å¯¼å…¥ç»“æœ
    """
    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_path = "data/analysis_results.db"
        result_db = ResultDatabase(db_path)
        
        imported_count = 0
        skipped_count = 0
        errors = []
        
        print(f"ğŸ“Š å¼€å§‹å¯¼å…¥ {len(data_records)} æ¡è®°å½•åˆ°æ•°æ®åº“...")
        
        for i, record in enumerate(data_records, 1):
            try:
                # æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºoriginal_idï¼‰
                if record.get('original_id') is not None:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒoriginal_idçš„è®°å½•
                    existing_results = result_db.get_analysis_results_by_original_id(record['original_id'])
                    if existing_results and len(existing_results) > 0:
                        print(f"â­ï¸  è·³è¿‡é‡å¤è®°å½• (original_id: {record['original_id']})")
                        skipped_count += 1
                        continue
                
                # å‡†å¤‡ä¿å­˜æ•°æ®
                save_data = {
                    'original_id': record.get('original_id'),
                    'title': record.get('title', 'æ— æ ‡é¢˜'),
                    'content': record.get('content', 'æ— å†…å®¹'),
                    'summary': record.get('summary', 'æ— æ‘˜è¦'),
                    'source': record.get('source', 'æœªçŸ¥æ¥æº'),
                    'publish_time': record.get('publish_time', 'æœªçŸ¥æ—¶é—´'),
                    'sentiment_level': record.get('sentiment_level', 'æœªçŸ¥'),
                    'sentiment_reason': record.get('sentiment_reason', 'æ— åŸå› '),
                    'companies': record.get('companies', ''),
                    'duplicate_id': record.get('duplicate_id', 'æ— '),
                    'duplication_rate': record.get('duplication_rate', 0.0),
                    'processing_time': record.get('processing_time', 0),
                    'tag_results': record.get('tag_results', {})
                }
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                save_result = result_db.save_analysis_result(save_data)
                
                if save_result['success']:
                    imported_count += 1
                    if i % 10 == 0:  # æ¯10æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        print(f"âœ… å·²å¯¼å…¥ {i}/{len(data_records)} æ¡è®°å½•")
                else:
                    errors.append(f"è®°å½• {i} ä¿å­˜å¤±è´¥: {save_result['message']}")
                    skipped_count += 1
                    
            except Exception as e:
                errors.append(f"è®°å½• {i} å¤„ç†å¤±è´¥: {str(e)}")
                skipped_count += 1
        
        print(f"âœ… æ•°æ®åº“å¯¼å…¥å®Œæˆï¼æˆåŠŸ: {imported_count}, è·³è¿‡: {skipped_count}")
        
        if errors:
            print(f"âš ï¸  å¯¼å…¥è¿‡ç¨‹ä¸­æœ‰ {len(errors)} ä¸ªé”™è¯¯:")
            for error in errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"   - {error}")
            if len(errors) > 5:
                print(f"   ... è¿˜æœ‰ {len(errors) - 5} ä¸ªé”™è¯¯")
        
        return {
            'success': True,
            'imported_count': imported_count,
            'skipped_count': skipped_count,
            'error_count': len(errors),
            'errors': errors,
            'message': f'æˆåŠŸå¯¼å…¥ {imported_count} æ¡è®°å½•ï¼Œè·³è¿‡ {skipped_count} æ¡è®°å½•'
        }
        
    except Exception as e:
        return {
            'success': False,
            'message': f'æ•°æ®åº“å¯¼å…¥å¤±è´¥: {str(e)}',
            'imported_count': 0,
            'skipped_count': 0,
            'error_count': 0,
            'errors': [str(e)]
        }

def deduplicate_database_records():
    """
    ç›´æ¥å¯¹æ•°æ®åº“ä¸­çš„è®°å½•è¿›è¡Œå»é‡ï¼Œç”¨äºç³»ç»Ÿè‡ªåŠ¨åŒ–æµç¨‹
    
    Returns:
        dict: å»é‡ç»“æœ
    """
    try:
        if not DATABASE_AVAILABLE:
            return {
                'success': False,
                'message': 'æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨',
                'duplicates_removed': 0
            }
        
        print("ğŸ”„ å¼€å§‹æ•°æ®åº“è‡ªåŠ¨å»é‡...")
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_path = "data/analysis_results.db"
        result_db = ResultDatabase(db_path)
        
        # è·å–æ‰€æœ‰è®°å½•
        all_results = result_db.get_analysis_results(page=1, page_size=10000)  # è·å–å¤§é‡æ•°æ®
        
        if not all_results['success']:
            return {
                'success': False,
                'message': f'è·å–æ•°æ®åº“è®°å½•å¤±è´¥: {all_results.get("message", "æœªçŸ¥é”™è¯¯")}',
                'duplicates_removed': 0
            }
        
        data_records = all_results['data']
        total_records = len(data_records)
        
        if total_records == 0:
            return {
                'success': True,
                'message': 'æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•éœ€è¦å»é‡',
                'duplicates_removed': 0
            }
        
        print(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {total_records} æ¡è®°å½•")
        
        # æŒ‰original_idè¿›è¡Œå»é‡ï¼Œä¿ç•™IDæœ€å°çš„è®°å½•
        unique_records = {}
        duplicates_count = 0
        records_to_delete = []
        
        for record in data_records:
            original_id = record.get('original_id')
            if original_id is None:
                continue
                
            if original_id not in unique_records:
                unique_records[original_id] = record
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ¯”è¾ƒIDï¼Œä¿ç•™IDè¾ƒå°çš„è®°å½•
                existing_id = unique_records[original_id].get('id')
                current_id = record.get('id')
                
                # å¤„ç†IDä¸ºNoneçš„æƒ…å†µ
                if existing_id is None:
                    existing_id = float('inf')
                if current_id is None:
                    current_id = float('inf')
                
                if current_id < existing_id:
                    # å½“å‰è®°å½•IDæ›´å°ï¼Œæ›¿æ¢ç°æœ‰è®°å½•
                    records_to_delete.append(unique_records[original_id])
                    unique_records[original_id] = record
                    duplicates_count += 1
                else:
                    # ç°æœ‰è®°å½•IDæ›´å°ï¼Œåˆ é™¤å½“å‰è®°å½•
                    records_to_delete.append(record)
                    duplicates_count += 1
        
        if duplicates_count == 0:
            return {
                'success': True,
                'message': 'æ•°æ®åº“ä¸­æ²¡æœ‰é‡å¤è®°å½•',
                'duplicates_removed': 0
            }
        
        print(f"âœ… å‘ç° {duplicates_count} æ¡é‡å¤è®°å½•ï¼Œå¼€å§‹åˆ é™¤...")
        
        # åˆ é™¤é‡å¤è®°å½•
        deleted_count = 0
        for record in records_to_delete:
            try:
                # è¿™é‡Œéœ€è¦å®ç°åˆ é™¤è®°å½•çš„æ–¹æ³•
                # ç”±äºResultDatabaseæ²¡æœ‰ç›´æ¥çš„åˆ é™¤æ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦æ‰©å±•å®ƒ
                delete_result = result_db.delete_analysis_result(record.get('id'))
                if delete_result['success']:
                    deleted_count += 1
                else:
                    print(f"âš ï¸  åˆ é™¤è®°å½• {record.get('id')} å¤±è´¥: {delete_result['message']}")
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤è®°å½• {record.get('id')} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        print(f"ğŸ—‘ï¸  æˆåŠŸåˆ é™¤ {deleted_count} æ¡é‡å¤è®°å½•")
        
        return {
            'success': True,
            'message': f'æ•°æ®åº“å»é‡å®Œæˆï¼Œåˆ é™¤ {deleted_count} æ¡é‡å¤è®°å½•',
            'duplicates_removed': deleted_count,
            'total_records': total_records,
            'unique_records': len(unique_records)
        }
        
    except Exception as e:
        error_msg = f'æ•°æ®åº“å»é‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'
        print(f"âŒ {error_msg}")
        return {
            'success': False,
            'message': error_msg,
            'duplicates_removed': 0
        }

def auto_export_after_dedup():
    """
    åœ¨å»é‡å®Œæˆåè‡ªåŠ¨å¯¼å‡ºæ•°æ®ï¼Œç”¨äºç³»ç»Ÿè‡ªåŠ¨åŒ–æµç¨‹
    
    Returns:
        dict: å¯¼å‡ºç»“æœ
    """
    try:
        if not DATABASE_AVAILABLE:
            return {
                'success': False,
                'message': 'æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨',
                'export_file': None
            }
        
        print("ğŸ“¤ å¼€å§‹è‡ªåŠ¨å¯¼å‡º...")
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_path = "data/analysis_results.db"
        result_db = ResultDatabase(db_path)
        
        # è·å–æ‰€æœ‰è®°å½•
        all_results = result_db.get_analysis_results(page=1, page_size=10000)
        
        if not all_results['success']:
            return {
                'success': False,
                'message': f'è·å–æ•°æ®åº“è®°å½•å¤±è´¥: {all_results.get("message", "æœªçŸ¥é”™è¯¯")}',
                'export_file': None
            }
        
        data_records = all_results['data']
        total_records = len(data_records)
        
        if total_records == 0:
            return {
                'success': False,
                'message': 'æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•å¯å¯¼å‡º',
                'export_file': None
            }
        
        print(f"ğŸ“Š å‡†å¤‡å¯¼å‡º {total_records} æ¡è®°å½•")
        
        # ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_dir = "exports"
        
        # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
        
        export_file = os.path.join(export_dir, f"auto_export_after_dedup_{timestamp}.json")
        
        # å¯¼å‡ºæ•°æ®
        with open(export_file, 'w', encoding='utf-8') as f:
            json.dump(data_records, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è‡ªåŠ¨å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶: {export_file}")
        print(f"ğŸ“ˆ å¯¼å‡ºè®°å½•æ•°: {total_records}")
        
        return {
            'success': True,
            'message': f'è‡ªåŠ¨å¯¼å‡ºå®Œæˆï¼Œå…±å¯¼å‡º {total_records} æ¡è®°å½•',
            'export_file': export_file,
            'total_records': total_records
        }
        
    except Exception as e:
        error_msg = f'è‡ªåŠ¨å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'
        print(f"âŒ {error_msg}")
        return {
            'success': False,
            'message': error_msg,
            'export_file': None
        }

def enhanced_export_data(export_format='json', include_metadata=True, filter_tags=None):
    """
    å¢å¼ºçš„æ•°æ®å¯¼å‡ºåŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œé€‰é¡¹
    
    Args:
        export_format: å¯¼å‡ºæ ¼å¼ ('json', 'csv', 'excel')
        include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®
        filter_tags: è¿‡æ»¤æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ['åŒä¸šç«äº‰', 'å…³è”äº¤æ˜“']
    
    Returns:
        dict: å¯¼å‡ºç»“æœ
    """
    try:
        if not DATABASE_AVAILABLE:
            return {
                'success': False,
                'message': 'æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨',
                'export_file': None
            }
        
        print(f"ğŸ“¤ å¼€å§‹å¢å¼ºå¯¼å‡º (æ ¼å¼: {export_format})...")
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_path = "data/analysis_results.db"
        result_db = ResultDatabase(db_path)
        
        # è·å–æ‰€æœ‰è®°å½•
        all_results = result_db.get_analysis_results(page=1, page_size=10000)
        
        if not all_results['success']:
            return {
                'success': False,
                'message': f'è·å–æ•°æ®åº“è®°å½•å¤±è´¥: {all_results.get("message", "æœªçŸ¥é”™è¯¯")}',
                'export_file': None
            }
        
        data_records = all_results['data']
        total_records = len(data_records)
        
        if total_records == 0:
            return {
                'success': False,
                'message': 'æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•å¯å¯¼å‡º',
                'export_file': None
            }
        
        # åº”ç”¨æ ‡ç­¾è¿‡æ»¤
        if filter_tags:
            filtered_records = []
            for record in data_records:
                for tag in filter_tags:
                    tag_field = f'tag_{tag}'
                    if tag_field in record and record[tag_field] == 'æ˜¯':
                        filtered_records.append(record)
                        break
            data_records = filtered_records
            print(f"ğŸ” æ ‡ç­¾è¿‡æ»¤åè®°å½•æ•°: {len(data_records)}")
        
        # ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_dir = "exports"
        
        # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
        
        if export_format == 'json':
            export_file = os.path.join(export_dir, f"enhanced_export_{timestamp}.json")
            export_result = export_to_json(data_records, export_file, include_metadata)
        elif export_format == 'csv':
            export_file = os.path.join(export_dir, f"enhanced_export_{timestamp}.csv")
            export_result = export_to_csv(data_records, export_file, include_metadata)
        elif export_format == 'excel':
            export_file = os.path.join(export_dir, f"enhanced_export_{timestamp}.xlsx")
            export_result = export_to_excel(data_records, export_file, include_metadata)
        else:
            return {
                'success': False,
                'message': f'ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {export_format}',
                'export_file': None
            }
        
        if export_result['success']:
            print(f"âœ… å¢å¼ºå¯¼å‡ºå®Œæˆï¼æ–‡ä»¶: {export_file}")
            print(f"ğŸ“ˆ å¯¼å‡ºè®°å½•æ•°: {len(data_records)}")
            
            return {
                'success': True,
                'message': f'å¢å¼ºå¯¼å‡ºå®Œæˆï¼Œå…±å¯¼å‡º {len(data_records)} æ¡è®°å½•',
                'export_file': export_file,
                'total_records': len(data_records),
                'format': export_format
            }
        else:
            return export_result
        
    except Exception as e:
        error_msg = f'å¢å¼ºå¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'
        print(f"âŒ {error_msg}")
        return {
            'success': False,
            'message': error_msg,
            'export_file': None
        }

def export_to_json(data_records, export_file, include_metadata=True):
    """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
    try:
        export_data = {
            'export_info': {
                'export_time': datetime.now().isoformat(),
                'total_records': len(data_records),
                'format': 'json'
            } if include_metadata else {},
            'data': data_records
        }
        
        with open(export_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return {'success': True, 'message': 'JSONå¯¼å‡ºæˆåŠŸ'}
        
    except Exception as e:
        return {'success': False, 'message': f'JSONå¯¼å‡ºå¤±è´¥: {str(e)}'}

def export_to_csv(data_records, export_file, include_metadata=True):
    """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
    try:
        import csv
        
        with open(export_file, 'w', encoding='utf-8', newline='') as f:
            if data_records:
                # è·å–æ‰€æœ‰å­—æ®µå
                fieldnames = list(data_records[0].keys())
                
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                # å†™å…¥æ•°æ®
                for record in data_records:
                    writer.writerow(record)
        
        return {'success': True, 'message': 'CSVå¯¼å‡ºæˆåŠŸ'}
        
    except Exception as e:
        return {'success': False, 'message': f'CSVå¯¼å‡ºå¤±è´¥: {str(e)}'}

def export_to_excel(data_records, export_file, include_metadata=True):
    """å¯¼å‡ºä¸ºExcelæ ¼å¼"""
    try:
        import pandas as pd
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data_records)
        
        # å†™å…¥Excelæ–‡ä»¶
        with pd.ExcelWriter(export_file, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='åˆ†æç»“æœ', index=False)
            
            # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œåˆ›å»ºå…ƒæ•°æ®å·¥ä½œè¡¨
            if include_metadata:
                metadata_df = pd.DataFrame([{
                    'å¯¼å‡ºæ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'æ€»è®°å½•æ•°': len(data_records),
                    'å¯¼å‡ºæ ¼å¼': 'Excel'
                }])
                metadata_df.to_excel(writer, sheet_name='å¯¼å‡ºä¿¡æ¯', index=False)
        
        return {'success': True, 'message': 'Excelå¯¼å‡ºæˆåŠŸ'}
        
    except Exception as e:
        return {'success': False, 'message': f'Excelå¯¼å‡ºå¤±è´¥: {str(e)}'}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ JSONæ–‡ä»¶å»é‡å·¥å…·")
    print("="*50)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
        output_file = sys.argv[2] if len(sys.argv) > 2 else None
        auto_import = True if len(sys.argv) <= 3 else (sys.argv[3].lower() != 'false')
    else:
        # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        input_file = r"C:\Users\anyut\Downloads\analysis_results_2025-08-26 (4).json"
        output_file = None
        auto_import = True
        
        # æ£€æŸ¥é»˜è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_file):
            print("âŒ é»˜è®¤æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æä¾›æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°")
            print("ä½¿ç”¨æ–¹æ³•: python deduplicate_any_json.py <è¾“å…¥æ–‡ä»¶è·¯å¾„> [è¾“å‡ºæ–‡ä»¶è·¯å¾„] [æ˜¯å¦å¯¼å…¥æ•°æ®åº“]")
            print("ç¤ºä¾‹: python deduplicate_any_json.py data.json")
            print("ç¤ºä¾‹: python deduplicate_any_json.py data.json output.json false")
            return
    
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    if output_file:
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"è‡ªåŠ¨å¯¼å…¥æ•°æ®åº“: {'æ˜¯' if auto_import else 'å¦'}")
    
    # æ‰§è¡Œå»é‡
    output_file = deduplicate_json_by_original_id(input_file, output_file, auto_import)
    
    if output_file:
        print(f"\nğŸ‰ å»é‡å®Œæˆï¼")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # éªŒè¯å»é‡ç»“æœ
        print("\nğŸ” æ­£åœ¨éªŒè¯å»é‡ç»“æœ...")
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                final_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡å¤çš„original_id
            original_ids = [record.get('original_id') for record in final_data if record.get('original_id') is not None]
            unique_original_ids = set(original_ids)
            
            if len(original_ids) == len(unique_original_ids):
                print("âœ… éªŒè¯é€šè¿‡ï¼šæ²¡æœ‰é‡å¤çš„original_id")
            else:
                print("âŒ éªŒè¯å¤±è´¥ï¼šä»ç„¶å­˜åœ¨é‡å¤çš„original_id")
                
        except Exception as e:
            print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    else:
        print("âŒ å»é‡å¤±è´¥")

if __name__ == "__main__":
    main()
