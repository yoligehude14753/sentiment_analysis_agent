#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»“æœAPIæ¥å£
æä¾›åˆ†æç»“æœçš„ä¿å­˜ã€æŸ¥è¯¢å’Œå¯¼å‡ºåŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from typing import List, Dict, Optional, Any
from pydantic import BaseModel
from database_manager import UnifiedDatabaseManager
from result_database_new import ResultDatabase
import logging

# åˆå§‹åŒ–æ•°æ®åº“å®ä¾‹
result_database = ResultDatabase('data/analysis_results.db')
from datetime import datetime
import json
import csv
import io
from comprehensive_fixes import ComprehensiveFixes
from auto_deduplicator import get_auto_deduplicator, get_database_auto_deduplicator

logger = logging.getLogger(__name__)

# åˆ›å»ºAPIè·¯ç”±å™¨
router = APIRouter(tags=["ç»“æœ"])

# æ•°æ®æ¨¡å‹
class ExportRequest(BaseModel):
    format: str = "csv"  # csv, json, excel
    options: Dict[str, bool] = {}
    auto_deduplicate: bool = True  # é»˜è®¤å¯ç”¨è‡ªåŠ¨å»é‡
    similarity_threshold: float = 0.85  # ç›¸ä¼¼åº¦é˜ˆå€¼

# ä¾èµ–æ³¨å…¥
def get_db_manager():
    """è·å–ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹"""
    return UnifiedDatabaseManager()

def get_result_db():
    """è·å–æ–°çš„ç»“æœæ•°æ®åº“å®ä¾‹"""
    return ResultDatabase('data/analysis_results.db')

@router.post("/save")
async def save_results(
    request: Dict[str, Any],
    db_manager: UnifiedDatabaseManager = Depends(get_db_manager)
):
    """ä¿å­˜åˆ†æç»“æœ"""
    try:
        results = request.get('results', [])
        database = request.get('database', 'default')
        
        if not results:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰ç»“æœæ•°æ®")
        
        # è·å–ç»“æœæ•°æ®åº“
        result_db = db_manager.get_result_database()
        
        # ä¿å­˜ç»“æœ
        saved_count = 0
        for result_data in results:
            try:
                # å‡†å¤‡ä¿å­˜çš„æ•°æ®
                save_data = {
                    'original_id': result_data.get('original_id'),
                    'title': result_data.get('title', ''),
                    'content': result_data.get('content', ''),
                    'source': result_data.get('source', ''),
                    'publish_time': result_data.get('publish_time', ''),
                    'sentiment_level': result_data.get('sentiment_level', ''),
                    'sentiment_reason': result_data.get('sentiment_reason', ''),
                    'tags': result_data.get('tags', ''),
                    'companies': result_data.get('companies', ''),
                    'processing_time': result_data.get('processing_time', 0),
                    'processed_at': result_data.get('processed_at', ''),
                    'analysis_status': result_data.get('analysis_status', 'completed')
                }
                
                # ä¿å­˜åˆ°ç»“æœæ•°æ®åº“
                save_result = result_db.save_analysis_result(save_data)
                if save_result['success']:
                    saved_count += 1
                else:
                    logger.warning(f"ä¿å­˜ç»“æœå¤±è´¥: {save_result['message']}")
                    
            except Exception as e:
                logger.error(f"ä¿å­˜å•æ¡ç»“æœå¤±è´¥: {str(e)}")
                continue
        
        return {
            "success": True,
            "message": f"æˆåŠŸä¿å­˜ {saved_count} æ¡åˆ†æç»“æœ",
            "saved_count": saved_count,
            "total_count": len(results)
        }
        
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}")

@router.get("/list")
async def list_results(
    page: int = 1,
    page_size: int = 50,
    result_db: ResultDatabase = Depends(get_result_db)
):
    """è·å–åˆ†æç»“æœåˆ—è¡¨"""
    try:
        # æŸ¥è¯¢ç»“æœ
        result = result_db.get_analysis_results(
            page=page,
            page_size=page_size
        )
        
        if result['success']:
            return {
                "success": True,
                "data": result['data'],
                "total": result['total'],
                "page": page,
                "page_size": page_size,
                "total_pages": result['total_pages']
            }
        else:
            raise HTTPException(status_code=500, detail=result['message'])
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}")

@router.get("/article/{article_id}")
async def get_article_detail(
    article_id: int,
    result_db: ResultDatabase = Depends(get_result_db)
):
    """è·å–å•æ¡æ–‡ç« è¯¦æƒ…"""
    try:
        # æŸ¥è¯¢å•æ¡ç»“æœ
        result = result_db.get_analysis_result_by_id(article_id)
        
        if result['success']:
            return {
                "success": True,
                "data": result['data']
            }
        else:
            raise HTTPException(status_code=404, detail=result['message'])
            
    except Exception as e:
        logger.error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")

@router.post("/export")
async def export_results(
    request: ExportRequest,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    session_id: Optional[str] = None,
    result_db: ResultDatabase = Depends(get_result_db)
):
    """å¯¼å‡ºåˆ†æç»“æœ"""
    try:
        # ä¼˜å…ˆæŒ‰ä¼šè¯IDå¯¼å‡º
        if session_id:
            result = result_db.get_results_by_session(session_id)
            if not result['success']:
                raise HTTPException(status_code=500, detail=result['message'])
            data = result['data']
            logger.info(f"æŒ‰ä¼šè¯IDå¯¼å‡º: {session_id}, å…± {len(data)} æ¡è®°å½•")
        else:
            # è·å–æ‰€æœ‰ç»“æœæ•°æ®
            result = result_db.get_analysis_results(
                page=1,
                page_size=10000  # è·å–å¤§é‡æ•°æ®ç”¨äºå¯¼å‡º
            )
            
            if not result['success']:
                raise HTTPException(status_code=500, detail=result['message'])
            
            data = result['data']
        
        # å¦‚æœæœ‰æ—¥æœŸè¿‡æ»¤å‚æ•°ï¼Œä½¿ç”¨SQLæŸ¥è¯¢è·å–æ•°æ®
        if start_date and end_date:
            try:
                import sqlite3
                with sqlite3.connect(result_db.db_path) as conn:
                    cursor = conn.cursor()
                    
                    # æ„å»ºæ—¥æœŸè¿‡æ»¤çš„SQLæŸ¥è¯¢
                    query = '''
                    SELECT * FROM sentiment_results 
                    WHERE publish_time BETWEEN ? AND ? 
                    ORDER BY id DESC
                    '''
                    cursor.execute(query, (start_date, end_date))
                    logger.info(f"JSONå¯¼å‡ºåº”ç”¨æ—¥æœŸè¿‡æ»¤: {start_date} åˆ° {end_date}")
                    
                    columns = [description[0] for description in cursor.description]
                    rows = cursor.fetchall()
                    data = [dict(zip(columns, row)) for row in rows]
                    logger.info(f"JSONå¯¼å‡ºé€šè¿‡SQLæŸ¥è¯¢è·å–åˆ° {len(data)} æ¡è®°å½•")
            except Exception as e:
                logger.error(f"JSONå¯¼å‡ºSQLæŸ¥è¯¢å¤±è´¥: {e}")
                raise HTTPException(status_code=500, detail="è·å–æ•°æ®å¤±è´¥")
        
        if not data:
            raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®")
        
        logger.info(f"å¯¼å‡ºå‰æ•°æ®é‡: {len(data)} æ¡")
        
        # ğŸ”§ è‡ªåŠ¨å»é‡å¤„ç†
        if request.auto_deduplicate and len(data) > 1:
            logger.info(f"ğŸ”„ å¼€å§‹è‡ªåŠ¨å»é‡å¤„ç†ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {request.similarity_threshold}")
            
            try:
                # è·å–è‡ªåŠ¨å»é‡å™¨
                deduplicator = get_auto_deduplicator()
                deduplicator.similarity_threshold = request.similarity_threshold
                
                # æ‰§è¡Œè‡ªåŠ¨å»é‡
                dedup_result = deduplicator.auto_deduplicate_export_data(data)
                
                if dedup_result['success']:
                    data = dedup_result['data']
                    stats = dedup_result['stats']
                    
                    logger.info(f"âœ… å»é‡å®Œæˆ: {stats['original_count']} â†’ {stats['final_count']} æ¡ (ç§»é™¤ {stats['removed_count']} æ¡é‡å¤)")
                    logger.info(f"å»é‡ç‡: {stats['deduplication_rate']:.2%}, å¤„ç†æ—¶é—´: {stats['processing_time_seconds']:.2f}s")
                else:
                    logger.warning(f"âš ï¸ å»é‡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {dedup_result['message']}")
                    
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨å»é‡è¿‡ç¨‹å‡ºé”™ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {str(e)}")
        else:
            logger.info("â„¹ï¸ è·³è¿‡è‡ªåŠ¨å»é‡ (æœªå¯ç”¨æˆ–æ•°æ®é‡ä¸è¶³)")
        
        # æ ¹æ®æ ¼å¼å¯¼å‡º
        if request.format.lower() == 'csv':
            return export_as_csv(data, request.options)
        elif request.format.lower() == 'json':
            return export_as_json(data, request.options)
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼")
            
    except Exception as e:
        logger.error(f"å¯¼å‡ºç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºç»“æœå¤±è´¥: {str(e)}")

def export_as_csv(data: List[Dict], options: Dict[str, bool]):
    """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
    try:
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        
        # æ ‡ç­¾åç§°åˆ—è¡¨
        tag_names = [
            "åŒä¸šç«äº‰", "è‚¡æƒä¸æ§åˆ¶æƒ", "å…³è”äº¤æ˜“", "å†å²æ²¿é©ä¸è‚¡ä¸œæ ¸æŸ¥", "é‡å¤§è¿æ³•è¿è§„",
            "æ”¶å…¥ä¸æˆæœ¬", "è´¢åŠ¡å†…æ§ä¸è§„èŒƒ", "å®¢æˆ·ä¸ä¾›åº”å•†", "èµ„äº§è´¨é‡ä¸å‡å€¼", "ç ”å‘ä¸æŠ€æœ¯",
            "å‹Ÿé›†èµ„é‡‘ç”¨é€”", "çªå‡»åˆ†çº¢ä¸å¯¹èµŒåè®®", "å¸‚åœºä¼ é—»ä¸è´Ÿé¢æŠ¥é“", "è¡Œä¸šæ”¿ç­–ä¸ç¯å¢ƒ"
        ]
        
        for item in data:
            row = {}
            
            if options.get('original', True):
                row['åŸå§‹ID'] = item.get('original_id', '')
                row['æ ‡é¢˜'] = item.get('title', '')
                row['å†…å®¹'] = item.get('content', '')
                row['æ‘˜è¦'] = item.get('summary', '')
                row['æ¥æº'] = item.get('source', '')
                row['å‘å¸ƒæ—¶é—´'] = item.get('publish_time', '')
            
            if options.get('sentiment', True):
                row['æƒ…æ„Ÿç­‰çº§'] = item.get('sentiment_level', '')
                row['æƒ…æ„ŸåŸå› '] = item.get('sentiment_reason', '')
            
            if options.get('tags', True):
                # æ·»åŠ æ‰€æœ‰æ ‡ç­¾å­—æ®µ
                for tag_name in tag_names:
                    tag_key = f'tag_{tag_name}'
                    reason_key = f'reason_{tag_name}'
                    row[f'æ ‡ç­¾_{tag_name}'] = item.get(tag_key, 'å¦')
                    row[f'åŸå› _{tag_name}'] = item.get(reason_key, 'æ— ')
            
            if options.get('companies', True):
                row['æ¶‰åŠä¼ä¸š'] = item.get('companies', '')
            
            if options.get('duplication', True):
                row['é‡å¤ID'] = item.get('duplicate_id', '')
                row['é‡å¤åº¦'] = item.get('duplication_rate', '')
            
            if options.get('processingTime', True):
                row['å¤„ç†æ—¶é—´(s)'] = item.get('processing_time', '')
            
            csv_data.append(row)
        
        # åˆ›å»ºCSVæ–‡ä»¶
        output = io.StringIO()
        if csv_data:
            writer = csv.DictWriter(output, fieldnames=csv_data[0].keys())
            writer.writeheader()
            writer.writerows(csv_data)
        
        # è¿”å›CSVæ–‡ä»¶
        output.seek(0)
        filename = f"analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        return StreamingResponse(
            iter([output.getvalue()]),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        
    except Exception as e:
        logger.error(f"CSVå¯¼å‡ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"CSVå¯¼å‡ºå¤±è´¥: {str(e)}")

def export_as_json(data: List[Dict], options: Dict[str, bool]):
    """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
    try:
        # æ ¹æ®é€‰é¡¹è¿‡æ»¤æ•°æ®
        filtered_data = []
        
        # æ ‡ç­¾åç§°åˆ—è¡¨
        tag_names = [
            "åŒä¸šç«äº‰", "è‚¡æƒä¸æ§åˆ¶æƒ", "å…³è”äº¤æ˜“", "å†å²æ²¿é©ä¸è‚¡ä¸œæ ¸æŸ¥", "é‡å¤§è¿æ³•è¿è§„",
            "æ”¶å…¥ä¸æˆæœ¬", "è´¢åŠ¡å†…æ§ä¸è§„èŒƒ", "å®¢æˆ·ä¸ä¾›åº”å•†", "èµ„äº§è´¨é‡ä¸å‡å€¼", "ç ”å‘ä¸æŠ€æœ¯",
            "å‹Ÿé›†èµ„é‡‘ç”¨é€”", "çªå‡»åˆ†çº¢ä¸å¯¹èµŒåè®®", "å¸‚åœºä¼ é—»ä¸è´Ÿé¢æŠ¥é“", "è¡Œä¸šæ”¿ç­–ä¸ç¯å¢ƒ"
        ]
        
        for item in data:
            filtered_item = {}
            
            if options.get('original', True):
                filtered_item['original_id'] = item.get('original_id')
                filtered_item['title'] = item.get('title')
                filtered_item['content'] = item.get('content')
                filtered_item['summary'] = item.get('summary')
                filtered_item['source'] = item.get('source')
                filtered_item['publish_time'] = item.get('publish_time')
            
            if options.get('sentiment', True):
                filtered_item['sentiment_level'] = item.get('sentiment_level')
                filtered_item['sentiment_reason'] = item.get('sentiment_reason')
            
            if options.get('tags', True):
                # æ·»åŠ æ‰€æœ‰æ ‡ç­¾å­—æ®µ
                tag_results = {}
                for tag_name in tag_names:
                    tag_key = f'tag_{tag_name}'
                    reason_key = f'reason_{tag_name}'
                    tag_results[tag_name] = {
                        'belongs': item.get(tag_key, 'å¦'),
                        'reason': item.get(reason_key, 'æ— ')
                    }
                filtered_item['tag_results'] = tag_results
            
            if options.get('companies', True):
                filtered_item['companies'] = item.get('companies')
            
            if options.get('duplication', True):
                filtered_item['duplicate_id'] = item.get('duplicate_id')
                filtered_item['duplication_rate'] = item.get('duplication_rate')
            
            if options.get('processingTime', True):
                filtered_item['processing_time'] = item.get('processing_time')
            
            filtered_data.append(filtered_item)
        
        # è¿”å›JSONæ–‡ä»¶
        filename = f"analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        return StreamingResponse(
            iter([json.dumps(filtered_data, ensure_ascii=False, indent=2)]),
            media_type="application/json",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        
    except Exception as e:
        logger.error(f"JSONå¯¼å‡ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"JSONå¯¼å‡ºå¤±è´¥: {str(e)}")

@router.get("/export/excel")
async def export_excel(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """å¯¼å‡ºExcelæ–‡ä»¶"""
    try:
        logger.info("å¼€å§‹Excelå¯¼å‡º")
        
        # ä½¿ç”¨å…¨å±€æ•°æ®åº“å®ä¾‹
        db = result_database
        
        # å¦‚æœæœ‰æ—¥æœŸè¿‡æ»¤å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨SQLæŸ¥è¯¢
        if start_date and end_date:
            results = []  # å¼ºåˆ¶ä½¿ç”¨SQLæŸ¥è¯¢
        else:
            # è·å–æ‰€æœ‰åˆ†æç»“æœ
            results_data = db.get_analysis_results(page_size=10000)  # è·å–å¤§é‡æ•°æ®
            results = results_data.get('results', [])
        
        # å¦‚æœéœ€è¦ä½¿ç”¨SQLæŸ¥è¯¢
        if (start_date and end_date) or not results:
            # å¦‚æœä½¿ç”¨æ–°æ–¹æ³•æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ç›´æ¥SQLæŸ¥è¯¢
            import sqlite3
            try:
                with sqlite3.connect(db.db_path) as conn:
                    cursor = conn.cursor()
                    
                    # æ„å»ºæ—¥æœŸè¿‡æ»¤çš„SQLæŸ¥è¯¢
                    if start_date and end_date:
                        query = '''
                        SELECT * FROM sentiment_results 
                        WHERE publish_time BETWEEN ? AND ? 
                        ORDER BY id DESC
                        '''
                        cursor.execute(query, (start_date, end_date))
                        logger.info(f"åº”ç”¨æ—¥æœŸè¿‡æ»¤: {start_date} åˆ° {end_date}")
                    else:
                        cursor.execute('SELECT * FROM sentiment_results ORDER BY id DESC')
                    
                    columns = [description[0] for description in cursor.description]
                    rows = cursor.fetchall()
                    results = [dict(zip(columns, row)) for row in rows]
                    logger.info(f"é€šè¿‡ç›´æ¥SQLæŸ¥è¯¢è·å–åˆ° {len(results)} æ¡è®°å½•")
            except Exception as e:
                logger.error(f"ç›´æ¥SQLæŸ¥è¯¢å¤±è´¥: {e}")
                raise HTTPException(status_code=500, detail="è·å–æ•°æ®å¤±è´¥")
        
        if not results:
            raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®")
        
        # åˆ›å»ºExcelæ–‡ä»¶
        from io import BytesIO
        import pandas as pd
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = []
        for result in results:
            row = {
                'ID': result.get('id', ''),
                'åŸå§‹ID': result.get('original_id', ''),
                'æ ‡é¢˜': result.get('title', ''),
                'å†…å®¹': result.get('content', ''),
                'æ‘˜è¦': result.get('summary', ''),
                'æ¥æº': result.get('source', ''),
                'å‘å¸ƒæ—¶é—´': result.get('publish_time', ''),
                'æƒ…æ„Ÿå€¾å‘': result.get('sentiment_level', ''),
                'æƒ…æ„ŸåŸå› ': result.get('sentiment_reason', ''),
                'ç›¸å…³å…¬å¸': result.get('companies', ''),
                'é‡å¤ID': result.get('duplicate_id', ''),
                'é‡å¤ç‡': result.get('duplication_rate', 0),
                'å¤„ç†æ—¶é—´(ms)': result.get('processing_time', 0),
                'åˆ†ææ—¶é—´': result.get('analysis_time', ''),
                'å¤„ç†çŠ¶æ€': result.get('processing_status', '')
            }
            
            # æ·»åŠ æ ‡ç­¾å­—æ®µ
            tag_labels = ['åŒä¸šç«äº‰', 'è‚¡æƒä¸æ§åˆ¶æƒ', 'å…³è”äº¤æ˜“', 'å†å²æ²¿é©ä¸è‚¡ä¸œæ ¸æŸ¥', 'é‡å¤§è¿æ³•è¿è§„',
                         'æ”¶å…¥ä¸æˆæœ¬', 'è´¢åŠ¡å†…æ§ä¸è§„èŒƒ', 'å®¢æˆ·ä¸ä¾›åº”å•†', 'èµ„äº§è´¨é‡ä¸å‡å€¼', 'ç ”å‘ä¸æŠ€æœ¯',
                         'å‹Ÿé›†èµ„é‡‘ç”¨é€”', 'çªå‡»åˆ†çº¢ä¸å¯¹èµŒåè®®', 'å¸‚åœºä¼ é—»ä¸è´Ÿé¢æŠ¥é“', 'è¡Œä¸šæ”¿ç­–ä¸ç¯å¢ƒ']
            
            for tag in tag_labels:
                tag_key = f'tag_{tag}'
                reason_key = f'reason_{tag}'
                row[f'æ ‡ç­¾-{tag}'] = result.get(tag_key, 'å¦')
                row[f'åŸå› -{tag}'] = result.get(reason_key, 'æ— ')
            
            export_data.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(export_data)
        
        # å†™å…¥Excel
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='åˆ†æç»“æœ', index=False)
        
        output.seek(0)
        excel_data = output.read()
        
        logger.info(f"Excelå¯¼å‡ºæˆåŠŸï¼Œå…±å¯¼å‡º {len(results)} æ¡è®°å½•ï¼Œæ–‡ä»¶å¤§å°: {len(excel_data)} å­—èŠ‚")
        
        # è¿”å›Excelæ–‡ä»¶
        from fastapi.responses import Response
        return Response(
            content=excel_data,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": "attachment; filename=sentiment_analysis_results.xlsx"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Excelå¯¼å‡ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Excelå¯¼å‡ºå¤±è´¥: {str(e)}")

@router.get("/export/json")
async def export_json(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    auto_deduplicate: bool = True,
    similarity_threshold: float = 0.85
):
    """å¯¼å‡ºJSONæ–‡ä»¶"""
    try:
        logger.info("å¼€å§‹JSONå¯¼å‡º")
        
        # ä½¿ç”¨å…¨å±€æ•°æ®åº“å®ä¾‹
        db = result_database
        
        # å¦‚æœæœ‰æ—¥æœŸè¿‡æ»¤å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨SQLæŸ¥è¯¢
        if start_date and end_date:
            results = []  # å¼ºåˆ¶ä½¿ç”¨SQLæŸ¥è¯¢
        else:
            # è·å–æ‰€æœ‰åˆ†æç»“æœ
            results_data = db.get_analysis_results(page_size=10000)  # è·å–å¤§é‡æ•°æ®
            results = results_data.get('results', [])
        
        # å¦‚æœéœ€è¦ä½¿ç”¨SQLæŸ¥è¯¢
        if (start_date and end_date) or not results:
            # å¦‚æœä½¿ç”¨æ–°æ–¹æ³•æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ç›´æ¥SQLæŸ¥è¯¢
            import sqlite3
            try:
                with sqlite3.connect(db.db_path) as conn:
                    cursor = conn.cursor()
                    
                    # æ„å»ºæ—¥æœŸè¿‡æ»¤çš„SQLæŸ¥è¯¢
                    if start_date and end_date:
                        query = '''
                        SELECT * FROM sentiment_results 
                        WHERE publish_time BETWEEN ? AND ? 
                        ORDER BY id DESC
                        '''
                        cursor.execute(query, (start_date, end_date))
                        logger.info(f"JSONå¯¼å‡ºåº”ç”¨æ—¥æœŸè¿‡æ»¤: {start_date} åˆ° {end_date}")
                    else:
                        cursor.execute('SELECT * FROM sentiment_results ORDER BY id DESC')
                    
                    columns = [description[0] for description in cursor.description]
                    rows = cursor.fetchall()
                    results = [dict(zip(columns, row)) for row in rows]
                    logger.info(f"JSONå¯¼å‡ºé€šè¿‡SQLæŸ¥è¯¢è·å–åˆ° {len(results)} æ¡è®°å½•")
            except Exception as e:
                logger.error(f"JSONå¯¼å‡ºSQLæŸ¥è¯¢å¤±è´¥: {e}")
                raise HTTPException(status_code=500, detail="è·å–æ•°æ®å¤±è´¥")
        
        if not results:
            raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®")
        
        logger.info(f"å¯¼å‡ºå‰æ•°æ®é‡: {len(results)} æ¡")
        
        # å‡†å¤‡JSONå¯¼å‡ºæ•°æ®
        export_data = []
        for result in results:
            item = {
                'id': result.get('id', ''),
                'original_id': result.get('original_id', ''),
                'title': result.get('title', ''),
                'content': result.get('content', ''),
                'summary': result.get('summary', ''),
                'source': result.get('source', ''),
                'publish_time': result.get('publish_time', ''),
                'sentiment_level': result.get('sentiment_level', ''),
                'sentiment_reason': result.get('sentiment_reason', ''),
                'companies': result.get('companies', ''),
                'duplicate_id': result.get('duplicate_id', ''),
                'duplication_rate': result.get('duplication_rate', 0),
                'processing_time': result.get('processing_time', 0),
                'analysis_time': result.get('analysis_time', ''),
                'processing_status': result.get('processing_status', '')
            }
            
            # æ·»åŠ æ ‡ç­¾å­—æ®µ
            tag_labels = ['åŒä¸šç«äº‰', 'è‚¡æƒä¸æ§åˆ¶æƒ', 'å…³è”äº¤æ˜“', 'å†å²æ²¿é©ä¸è‚¡ä¸œæ ¸æŸ¥', 'é‡å¤§è¿æ³•è¿è§„',
                         'æ”¶å…¥ä¸æˆæœ¬', 'è´¢åŠ¡å†…æ§ä¸è§„èŒƒ', 'å®¢æˆ·ä¸ä¾›åº”å•†', 'èµ„äº§è´¨é‡ä¸å‡å€¼', 'ç ”å‘ä¸æŠ€æœ¯',
                         'å‹Ÿé›†èµ„é‡‘ç”¨é€”', 'çªå‡»åˆ†çº¢ä¸å¯¹èµŒåè®®', 'å¸‚åœºä¼ é—»ä¸è´Ÿé¢æŠ¥é“', 'è¡Œä¸šæ”¿ç­–ä¸ç¯å¢ƒ']
            
            tag_results = {}
            for tag in tag_labels:
                tag_key = f'tag_{tag}'
                reason_key = f'reason_{tag}'
                tag_results[tag] = {
                    'belongs': result.get(tag_key, 'å¦'),
                    'reason': result.get(reason_key, 'æ— ')
                }
            
            item['tag_results'] = tag_results
            export_data.append(item)
        
        # ğŸ”§ è‡ªåŠ¨å»é‡å¤„ç†
        if auto_deduplicate and len(export_data) > 1:
            logger.info(f"ğŸ”„ å¼€å§‹è‡ªåŠ¨å»é‡å¤„ç†ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
            
            try:
                # è·å–è‡ªåŠ¨å»é‡å™¨
                deduplicator = get_auto_deduplicator()
                deduplicator.similarity_threshold = similarity_threshold
                
                # æ‰§è¡Œè‡ªåŠ¨å»é‡
                dedup_result = deduplicator.auto_deduplicate_export_data(export_data)
                
                if dedup_result['success']:
                    export_data = dedup_result['data']
                    stats = dedup_result['stats']
                    
                    logger.info(f"âœ… å»é‡å®Œæˆ: {stats['original_count']} â†’ {stats['final_count']} æ¡ (ç§»é™¤ {stats['removed_count']} æ¡é‡å¤)")
                    logger.info(f"å»é‡ç‡: {stats['deduplication_rate']:.2%}, å¤„ç†æ—¶é—´: {stats['processing_time_seconds']:.2f}s")
                else:
                    logger.warning(f"âš ï¸ å»é‡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {dedup_result['message']}")
                    
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨å»é‡è¿‡ç¨‹å‡ºé”™ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {str(e)}")
        else:
            logger.info("â„¹ï¸ è·³è¿‡è‡ªåŠ¨å»é‡ (æœªå¯ç”¨æˆ–æ•°æ®é‡ä¸è¶³)")
        
        # è¿”å›JSONæ–‡ä»¶
        dedup_suffix = "_deduplicated" if auto_deduplicate and len(export_data) > 1 else ""
        filename = f"sentiment_analysis_results{dedup_suffix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        logger.info(f"JSONå¯¼å‡ºæˆåŠŸï¼Œå…±å¯¼å‡º {len(export_data)} æ¡è®°å½•")
        
        return StreamingResponse(
            iter([json.dumps(export_data, ensure_ascii=False, indent=2)]),
            media_type="application/json",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"JSONå¯¼å‡ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"JSONå¯¼å‡ºå¤±è´¥: {str(e)}")

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "results_api"
    }

# æ•°æ®åº“ç®¡ç†ç›¸å…³API
@router.get("/database/stats")
async def get_database_stats():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        fixer = ComprehensiveFixes()
        stats = fixer.get_database_stats()
        return {
            "success": True,
            "data": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.post("/database/fix-summaries")
async def fix_empty_summaries():
    """ä¿®å¤ç©ºæ‘˜è¦"""
    try:
        fixer = ComprehensiveFixes()
        fixer.fix_empty_summaries()
        return {
            "success": True,
            "message": "æ‘˜è¦ä¿®å¤å®Œæˆ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ä¿®å¤æ‘˜è¦å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¿®å¤æ‘˜è¦å¤±è´¥: {str(e)}")

@router.post("/database/detect-duplicates")
async def detect_duplicates(similarity_threshold: float = 0.8):
    """æ£€æµ‹å¹¶æ ‡è®°é‡å¤æ•°æ®"""
    try:
        if similarity_threshold < 0.1 or similarity_threshold > 1.0:
            raise HTTPException(status_code=400, detail="ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0.1-1.0ä¹‹é—´")
        
        fixer = ComprehensiveFixes()
        duplicate_count = fixer.detect_duplicates_and_update(similarity_threshold)
        
        return {
            "success": True,
            "message": f"é‡å¤æ£€æµ‹å®Œæˆï¼Œæ‰¾åˆ° {duplicate_count} å¯¹é‡å¤æ•°æ®",
            "duplicate_pairs": duplicate_count,
            "similarity_threshold": similarity_threshold,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"æ£€æµ‹é‡å¤æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ£€æµ‹é‡å¤æ•°æ®å¤±è´¥: {str(e)}")

@router.post("/database/auto-deduplicate")
async def auto_deduplicate_database(similarity_threshold: float = 0.85):
    """è‡ªåŠ¨å»é‡æ•°æ®åº“è®°å½•"""
    try:
        if similarity_threshold < 0.1 or similarity_threshold > 1.0:
            raise HTTPException(status_code=400, detail="ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0.1-1.0ä¹‹é—´")
        
        # è·å–æ•°æ®åº“è‡ªåŠ¨å»é‡å™¨
        db_deduplicator = get_database_auto_deduplicator()
        db_deduplicator.similarity_threshold = similarity_threshold
        
        # æ‰§è¡Œè‡ªåŠ¨å»é‡
        result = db_deduplicator.auto_deduplicate_database()
        
        if result['success']:
            return {
                "success": True,
                "message": result['message'],
                "stats": result['stats'],
                "similarity_threshold": similarity_threshold,
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail=result['message'])
            
    except Exception as e:
        logger.error(f"æ•°æ®åº“è‡ªåŠ¨å»é‡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ•°æ®åº“è‡ªåŠ¨å»é‡å¤±è´¥: {str(e)}")

@router.post("/database/clean-duplicates")
async def clean_duplicate_records(keep_strategy: str = "first"):
    """æ¸…ç†é‡å¤è®°å½•"""
    try:
        if keep_strategy not in ["first", "latest"]:
            raise HTTPException(status_code=400, detail="ä¿ç•™ç­–ç•¥å¿…é¡»æ˜¯ 'first' æˆ– 'latest'")
        
        fixer = ComprehensiveFixes()
        deleted_count = fixer.clean_duplicate_records(keep_strategy)
        
        return {
            "success": True,
            "message": f"é‡å¤æ•°æ®æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} æ¡è®°å½•",
            "deleted_count": deleted_count,
            "keep_strategy": keep_strategy,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"æ¸…ç†é‡å¤æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†é‡å¤æ•°æ®å¤±è´¥: {str(e)}")

@router.post("/export/deduplicated")
async def export_deduplicated_data(
    session_id: Optional[str] = None,
    format: str = "json"
):
    """å¯¼å‡ºå»é‡åçš„æ•°æ®"""
    try:
        if format not in ["json", "csv"]:
            raise HTTPException(status_code=400, detail="æ ¼å¼å¿…é¡»æ˜¯ 'json' æˆ– 'csv'")
        
        fixer = ComprehensiveFixes()
        
        if format == "json":
            # JSONå¯¼å‡º
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"deduplicated_results_{timestamp}.json"
            
            export_result = fixer.export_deduplicated_data(session_id, filename)
            
            if export_result.get('success'):
                # è¯»å–ç”Ÿæˆçš„æ–‡ä»¶
                with open(filename, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                return StreamingResponse(
                    iter([content]),
                    media_type="application/json",
                    headers={"Content-Disposition": f"attachment; filename={filename}"}
                )
            else:
                raise HTTPException(status_code=500, detail=export_result.get('error', 'å¯¼å‡ºå¤±è´¥'))
        
        else:
            # CSVå¯¼å‡º (æœªæ¥å®ç°)
            raise HTTPException(status_code=501, detail="CSVå»é‡å¯¼å‡ºåŠŸèƒ½å¼€å‘ä¸­")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å‡ºå»é‡æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")
