"""
èˆ†æƒ…åˆ†æç³»ç»Ÿç»¼åˆä¿®å¤å·¥å…· V2.0
ä¿®å¤ç‰ˆæœ¬ - åŸºäºoriginal_idçš„æ­£ç¡®å»é‡é€»è¾‘
"""

import sqlite3
import json
import pandas as pd
from datetime import datetime
from typing import Optional, Dict, List, Any
import hashlib
import os
import logging

logger = logging.getLogger(__name__)

class ComprehensiveFixesV2:
    """ç»¼åˆä¿®å¤å·¥å…·ç±» V2.0 - ä¿®å¤ç‰ˆæœ¬"""
    
    def __init__(self, db_path: str = "data/analysis_results.db"):
        self.db_path = db_path
        
    def generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼"""
        if not content:
            return ""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def fix_empty_summaries(self):
        """ä¿®å¤ç©ºæ‘˜è¦"""
        print("ğŸ”§ å¼€å§‹ä¿®å¤ç©ºæ‘˜è¦...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æŸ¥æ‰¾ç©ºæ‘˜è¦è®°å½•
                cursor.execute("""
                    SELECT id, title, content 
                    FROM sentiment_results 
                    WHERE summary IS NULL OR summary = '' OR summary = 'None'
                """)
                
                empty_summary_records = cursor.fetchall()
                print(f"å‘ç° {len(empty_summary_records)} æ¡ç©ºæ‘˜è¦è®°å½•")
                
                fixed_count = 0
                for record_id, title, content in empty_summary_records:
                    # ç”Ÿæˆç®€å•æ‘˜è¦ï¼ˆä½¿ç”¨æ ‡é¢˜æˆ–å†…å®¹å‰100å­—ç¬¦ï¼‰
                    if title and title.strip():
                        summary = title.strip()[:100]
                    elif content and content.strip():
                        # æ¸…ç†HTMLæ ‡ç­¾å¹¶å–å‰100å­—ç¬¦
                        import re
                        clean_content = re.sub(r'<[^>]+>', '', content)
                        clean_content = clean_content.replace('\n', '').replace('\r', '').strip()
                        summary = clean_content[:100] if clean_content else "æ— æ‘˜è¦"
                    else:
                        summary = "æ— æ‘˜è¦"
                    
                    # æ›´æ–°æ‘˜è¦
                    cursor.execute("""
                        UPDATE sentiment_results 
                        SET summary = ? 
                        WHERE id = ?
                    """, (summary, record_id))
                    
                    fixed_count += 1
                
                conn.commit()
                print(f"âœ… æˆåŠŸä¿®å¤ {fixed_count} æ¡ç©ºæ‘˜è¦è®°å½•")
                return fixed_count
                
        except Exception as e:
            print(f"âŒ ä¿®å¤ç©ºæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
            return 0
    
    def detect_and_mark_duplicates(self):
        """æ£€æµ‹å¹¶æ ‡è®°é‡å¤æ•°æ®"""
        print("ğŸ” å¼€å§‹æ£€æµ‹é‡å¤æ•°æ®...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # è·å–æ‰€æœ‰è®°å½•
                cursor.execute("SELECT id, original_id, content FROM sentiment_results ORDER BY id")
                all_records = cursor.fetchall()
                
                print(f"æ€»è®°å½•æ•°: {len(all_records)}")
                
                # æŒ‰original_idåˆ†ç»„æ£€æµ‹é‡å¤
                original_id_groups = {}
                for record_id, original_id, content in all_records:
                    if original_id not in original_id_groups:
                        original_id_groups[original_id] = []
                    original_id_groups[original_id].append((record_id, content))
                
                duplicate_pairs = 0
                marked_count = 0
                
                for original_id, records in original_id_groups.items():
                    if len(records) > 1:
                        # æœ‰é‡å¤ï¼Œä¿ç•™ç¬¬ä¸€æ¡ï¼Œæ ‡è®°å…¶ä»–ä¸ºé‡å¤
                        primary_record = records[0]  # IDæœ€å°çš„ä½œä¸ºä¸»è®°å½•
                        
                        for i, (record_id, content) in enumerate(records[1:], 1):
                            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„åŸºäºå†…å®¹é•¿åº¦çš„ç›¸ä¼¼åº¦ï¼‰
                            similarity = 1.0  # åŒoriginal_idè®¤ä¸º100%é‡å¤
                            
                            # æ›´æ–°é‡å¤æ ‡è®°
                            cursor.execute("""
                                UPDATE sentiment_results 
                                SET duplicate_id = ?, duplication_rate = ?
                                WHERE id = ?
                            """, (primary_record[0], similarity, record_id))
                            
                            duplicate_pairs += 1
                            marked_count += 1
                
                conn.commit()
                print(f"âœ… æ£€æµ‹å®Œæˆï¼Œå‘ç° {duplicate_pairs} å¯¹é‡å¤æ•°æ®ï¼Œæ ‡è®°äº† {marked_count} æ¡é‡å¤è®°å½•")
                return duplicate_pairs
                
        except Exception as e:
            print(f"âŒ æ£€æµ‹é‡å¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return 0
    
    def export_deduplicated_data(self, session_id: Optional[str] = None, 
                               output_format: str = "both", 
                               keep_strategy: str = "first"):
        """å¯¼å‡ºå»é‡åçš„æ•°æ®ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        print("ğŸ“¤ å¼€å§‹å¯¼å‡ºå»é‡åçš„æ•°æ®...")
        print(f"ğŸ”§ å»é‡ç­–ç•¥: åŸºäºoriginal_idï¼Œä¿ç•™{keep_strategy}æ¡è®°å½•")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                if session_id:
                    query = '''
                        SELECT * FROM sentiment_results 
                        WHERE session_id = ?
                        ORDER BY id ASC
                    '''
                    cursor.execute(query, (session_id,))
                else:
                    query = '''
                        SELECT * FROM sentiment_results 
                        ORDER BY id ASC
                    '''
                    cursor.execute(query)
                
                columns = [description[0] for description in cursor.description]
                rows = cursor.fetchall()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                all_data = []
                for row in rows:
                    data_dict = dict(zip(columns, row))
                    all_data.append(data_dict)
                
                print(f"ä»æ•°æ®åº“è·å–äº† {len(all_data)} æ¡è®°å½•")
                
                if not all_data:
                    return {'success': False, 'message': 'æ²¡æœ‰æ‰¾åˆ°è®°å½•', 'files': []}
                
                # âœ… ä¿®å¤ï¼šåŸºäºoriginal_idå»é‡
                original_id_groups = {}
                for data in all_data:
                    original_id = data.get('original_id')
                    if original_id is None:
                        original_id = f"id_{data.get('id')}"
                    
                    if original_id not in original_id_groups:
                        original_id_groups[original_id] = []
                    original_id_groups[original_id].append(data)
                
                # ç»Ÿè®¡é‡å¤æƒ…å†µ
                duplicate_groups = {k: v for k, v in original_id_groups.items() if len(v) > 1}
                total_duplicates = sum(len(v) - 1 for v in duplicate_groups.values())
                
                print(f"ğŸ“Š å»é‡ç»Ÿè®¡:")
                print(f"   å”¯ä¸€original_idæ•°: {len(original_id_groups)}")
                print(f"   é‡å¤çš„original_idæ•°: {len(duplicate_groups)}")
                print(f"   é‡å¤è®°å½•æ€»æ•°: {total_duplicates}")
                
                if duplicate_groups:
                    print(f"ğŸ” é‡å¤è®°å½•è¯¦æƒ…ï¼ˆæ˜¾ç¤ºå‰5ä¸ªï¼‰:")
                    for original_id, group in list(duplicate_groups.items())[:5]:
                        print(f"   original_id '{original_id}': {len(group)} æ¡è®°å½•")
                
                # æ‰§è¡Œå»é‡
                deduplicated_data = []
                for original_id, group in original_id_groups.items():
                    if keep_strategy == "first":
                        selected_record = min(group, key=lambda x: x.get('id', 0))
                    elif keep_strategy == "last":
                        selected_record = max(group, key=lambda x: x.get('id', 0))
                    else:
                        selected_record = group[0]
                    
                    deduplicated_data.append(selected_record)
                
                print(f"âœ… å»é‡å®Œæˆ: {len(all_data)} -> {len(deduplicated_data)} æ¡è®°å½•")
                
                # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
                os.makedirs("exports", exist_ok=True)
                
                # å¯¼å‡ºæ–‡ä»¶
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                exported_files = []
                
                if output_format in ["json", "both"]:
                    json_file = f"exports/deduplicated_results_v2_{timestamp}.json"
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(deduplicated_data, f, ensure_ascii=False, indent=2, default=str)
                    exported_files.append(json_file)
                    print(f"ğŸ’¾ JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
                
                if output_format in ["csv", "both"]:
                    csv_file = f"exports/deduplicated_results_v2_{timestamp}.csv"
                    df = pd.DataFrame(deduplicated_data)
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    exported_files.append(csv_file)
                    print(f"ğŸ’¾ CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
                
                return {
                    'success': True,
                    'message': f'æˆåŠŸå¯¼å‡ºå»é‡æ•°æ®',
                    'files': exported_files,
                    'stats': {
                        'original_count': len(all_data),
                        'deduplicated_count': len(deduplicated_data),
                        'removed_count': len(all_data) - len(deduplicated_data)
                    }
                }
                
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå»é‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return {'success': False, 'message': f'å¯¼å‡ºå¤±è´¥: {str(e)}', 'files': []}
    
    def clean_duplicate_records(self, keep_strategy: str = "first"):
        """æ¸…ç†é‡å¤è®°å½•ï¼ˆç‰©ç†åˆ é™¤ï¼‰"""
        print(f"ğŸ—‘ï¸ å¼€å§‹æ¸…ç†é‡å¤è®°å½•ï¼ˆä¿ç•™ç­–ç•¥: {keep_strategy}ï¼‰...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # è·å–æ‰€æœ‰è®°å½•
                cursor.execute("SELECT id, original_id FROM sentiment_results ORDER BY id")
                all_records = cursor.fetchall()
                
                # æŒ‰original_idåˆ†ç»„
                original_id_groups = {}
                for record_id, original_id in all_records:
                    if original_id not in original_id_groups:
                        original_id_groups[original_id] = []
                    original_id_groups[original_id].append(record_id)
                
                # æ‰¾å‡ºè¦åˆ é™¤çš„è®°å½•
                records_to_delete = []
                for original_id, record_ids in original_id_groups.items():
                    if len(record_ids) > 1:
                        # æœ‰é‡å¤ï¼Œå†³å®šä¿ç•™å“ªä¸€æ¡
                        if keep_strategy == "first":
                            keep_id = min(record_ids)
                        elif keep_strategy == "last":
                            keep_id = max(record_ids)
                        else:
                            keep_id = record_ids[0]
                        
                        # å…¶ä»–çš„éƒ½è¦åˆ é™¤
                        for record_id in record_ids:
                            if record_id != keep_id:
                                records_to_delete.append(record_id)
                
                print(f"å‡†å¤‡åˆ é™¤ {len(records_to_delete)} æ¡é‡å¤è®°å½•")
                
                if records_to_delete:
                    # æ‰§è¡Œåˆ é™¤
                    placeholders = ','.join(['?'] * len(records_to_delete))
                    cursor.execute(f"DELETE FROM sentiment_results WHERE id IN ({placeholders})", 
                                 records_to_delete)
                    
                    conn.commit()
                    print(f"âœ… æˆåŠŸåˆ é™¤ {len(records_to_delete)} æ¡é‡å¤è®°å½•")
                else:
                    print("âœ… æ²¡æœ‰å‘ç°é‡å¤è®°å½•")
                
                return len(records_to_delete)
                
        except Exception as e:
            print(f"âŒ æ¸…ç†é‡å¤è®°å½•æ—¶å‡ºé”™: {str(e)}")
            return 0
    
    def run_all_fixes(self):
        """è¿è¡Œæ‰€æœ‰ä¿®å¤æ“ä½œ"""
        print("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰ä¿®å¤æ“ä½œ...")
        print("="*60)
        
        results = {}
        
        # 1. ä¿®å¤ç©ºæ‘˜è¦
        print("æ­¥éª¤1: ä¿®å¤ç©ºæ‘˜è¦")
        results['summary_fixes'] = self.fix_empty_summaries()
        print()
        
        # 2. æ£€æµ‹é‡å¤æ•°æ®
        print("æ­¥éª¤2: æ£€æµ‹é‡å¤æ•°æ®")
        results['duplicate_detection'] = self.detect_and_mark_duplicates()
        print()
        
        # 3. å¯¼å‡ºå»é‡æ•°æ®
        print("æ­¥éª¤3: å¯¼å‡ºå»é‡æ•°æ®")
        export_result = self.export_deduplicated_data(output_format="both")
        results['export_result'] = export_result
        print()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        print("æ­¥éª¤4: ç”Ÿæˆä¿®å¤æŠ¥å‘Š")
        print("="*60)
        print("ğŸ“ˆ ä¿®å¤ç»“æœæ±‡æ€»:")
        print(f"   ä¿®å¤ç©ºæ‘˜è¦: {results['summary_fixes']} æ¡")
        print(f"   æ£€æµ‹é‡å¤æ•°æ®: {results['duplicate_detection']} å¯¹")
        
        if export_result['success']:
            stats = export_result['stats']
            print(f"   å¯¼å‡ºç»Ÿè®¡:")
            print(f"     åŸå§‹è®°å½•æ•°: {stats['original_count']}")
            print(f"     å»é‡åè®°å½•æ•°: {stats['deduplicated_count']}")
            print(f"     åˆ é™¤é‡å¤è®°å½•æ•°: {stats['removed_count']}")
            print(f"   å¯¼å‡ºæ–‡ä»¶: {export_result['files']}")
        else:
            print(f"   å¯¼å‡ºå¤±è´¥: {export_result['message']}")
        
        print("âœ… æ‰€æœ‰ä¿®å¤æ“ä½œå®Œæˆ!")
        return results

def test_csv_deduplication():
    """æµ‹è¯•CSVæ–‡ä»¶å»é‡"""
    csv_file = r"C:\Users\anyut\Downloads\analysis_results_2025-08-26 (4).csv"
    
    if not os.path.exists(csv_file):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return
    
    print("ğŸ§ª CSVæ–‡ä»¶å»é‡æµ‹è¯•")
    print("="*50)
    
    try:
        # è¯»å–CSV
        df = pd.read_csv(csv_file, encoding='utf-8')
        print(f"ğŸ“Š åŸå§‹CSVæ–‡ä»¶: {len(df)} æ¡è®°å½•")
        
        # æ£€æŸ¥å­—æ®µ
        if 'åŸå§‹ID' in df.columns:
            original_id_col = 'åŸå§‹ID'
        else:
            print("âŒ æœªæ‰¾åˆ°'åŸå§‹ID'å­—æ®µ")
            return
        
        # ç»Ÿè®¡é‡å¤
        duplicate_stats = df[original_id_col].value_counts()
        duplicates = duplicate_stats[duplicate_stats > 1]
        
        print(f"ğŸ“ˆ é‡å¤æƒ…å†µ:")
        print(f"   å”¯ä¸€original_idæ•°: {len(duplicate_stats)}")
        print(f"   æœ‰é‡å¤çš„original_idæ•°: {len(duplicates)}")
        print(f"   é‡å¤è®°å½•æ€»æ•°: {duplicates.sum()}")
        
        # æ‰§è¡Œå»é‡
        deduplicated_df = df.drop_duplicates(subset=[original_id_col], keep='first')
        
        print(f"\nâœ… å»é‡ç»“æœ:")
        print(f"   å»é‡åè®°å½•æ•°: {len(deduplicated_df)}")
        print(f"   åˆ é™¤é‡å¤è®°å½•æ•°: {len(df) - len(deduplicated_df)}")
        
        # ä¿å­˜å»é‡æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"exports/csv_fixed_dedup_{timestamp}.csv"
        os.makedirs("exports", exist_ok=True)
        
        deduplicated_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ä¿®å¤åçš„CSVå·²ä¿å­˜: {output_file}")
        
    except Exception as e:
        print(f"âŒ å¤„ç†CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    print("ğŸ”§ èˆ†æƒ…åˆ†æç³»ç»Ÿç»¼åˆä¿®å¤å·¥å…· V2.0")
    print("="*80)
    
    # æ•°æ®åº“ä¿®å¤
    fixer = ComprehensiveFixesV2()
    db_results = fixer.run_all_fixes()
    
    print("\n" + "="*80)
    
    # CSVæ–‡ä»¶å»é‡æµ‹è¯•
    test_csv_deduplication()
    
    print("\nğŸ‰ æ‰€æœ‰ä¿®å¤å’Œæµ‹è¯•å®Œæˆï¼")
